//
//  CameraPreviewView.swift
//  USBExternalCamera
//
//  Created by EUN YEON on 5/25/25.
//

import AVFoundation
import HaishinKit
import SwiftUI
import UIKit
import Foundation

// MARK: - String Extension for Regex

extension String {
  func matches(for regex: String) -> [String] {
    do {
      let regex = try NSRegularExpression(pattern: regex)
      let results = regex.matches(in: self, range: NSRange(self.startIndex..., in: self))
      return results.map {
        String(self[Range($0.range, in: self)!])
      }
    } catch {
      return []
    }
  }
}

/// **ì‹¤ì œ HaishinKit RTMP ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ ì¹´ë©”ë¼ ë¯¸ë¦¬ë³´ê¸°**
struct CameraPreviewView: UIViewRepresentable {
  let session: AVCaptureSession
  var streamViewModel: LiveStreamViewModel?
  var haishinKitManager: HaishinKitManager?

  init(
    session: AVCaptureSession, streamViewModel: LiveStreamViewModel? = nil,
    haishinKitManager: HaishinKitManager? = nil
  ) {
    self.session = session
    self.streamViewModel = streamViewModel
    self.haishinKitManager = haishinKitManager
  }

  func makeUIView(context: Context) -> UIView {
    // í•­ìƒ AVCaptureVideoPreviewLayer ì‚¬ìš©í•˜ì—¬ ì¹´ë©”ë¼ ë¯¸ë¦¬ë³´ê¸° ìœ ì§€
    // HaishinKitì€ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ìŠ¤íŠ¸ë¦¬ë°ë§Œ ì²˜ë¦¬
    let view = CameraPreviewUIView()
    view.captureSession = session
    view.haishinKitManager = haishinKitManager
    return view
  }

  func updateUIView(_ uiView: UIView, context: Context) {
    if let previewView = uiView as? CameraPreviewUIView {
      // ì„¸ì…˜ì´ë‚˜ ë§¤ë‹ˆì €ê°€ ë³€ê²½ëœ ê²½ìš°ì—ë§Œ ì—…ë°ì´íŠ¸
      let sessionChanged = previewView.captureSession !== session
      let managerChanged = previewView.haishinKitManager !== haishinKitManager

      if sessionChanged {
        logInfo("ìº¡ì²˜ ì„¸ì…˜ ë³€ê²½ ê°ì§€ - ì—…ë°ì´íŠ¸", category: .camera)
        previewView.captureSession = session
      }

      if managerChanged {
        logInfo("HaishinKit ë§¤ë‹ˆì € ë³€ê²½ ê°ì§€ - ì—…ë°ì´íŠ¸", category: .camera)
        previewView.haishinKitManager = haishinKitManager
      }

      // í”„ë¦¬ë·° ìƒˆë¡œê³ ì¹¨ì€ í•˜ì§€ ì•ŠìŒ (ì•ˆì •ì„± í–¥ìƒ)
      logInfo("ì—…ë°ì´íŠ¸ ì™„ë£Œ - í”„ë¦¬ë·° ìƒˆë¡œê³ ì¹¨ ê±´ë„ˆëœ€", category: .camera)
    }
  }

  // MARK: - Screen Capture Control Methods

  /// í™”ë©´ ìº¡ì²˜ ì†¡ì¶œ ì‹œì‘ (ì™¸ë¶€ì—ì„œ í˜¸ì¶œ ê°€ëŠ¥)
  func startScreenCapture() {
    // UIViewRepresentableì—ì„œ UIViewì— ì ‘ê·¼í•˜ëŠ” ë°©ë²•ì´ ì œí•œì ì´ë¯€ë¡œ
    // HaishinKitManagerë¥¼ í†µí•´ ì œì–´í•˜ëŠ” ê²ƒì„ ê¶Œì¥
    logInfo("í™”ë©´ ìº¡ì²˜ ìš”ì²­ë¨ - HaishinKitManager ì‚¬ìš© ê¶Œì¥", category: .streaming)
  }

  /// í™”ë©´ ìº¡ì²˜ ì†¡ì¶œ ì¤‘ì§€ (ì™¸ë¶€ì—ì„œ í˜¸ì¶œ ê°€ëŠ¥)
  func stopScreenCapture() {
    logInfo("í™”ë©´ ìº¡ì²˜ ì¤‘ì§€ ìš”ì²­ë¨", category: .streaming)

    // í™”ë©´ ìº¡ì²˜ ì¤‘ì§€ ì•Œë¦¼ ì „ì†¡
    DispatchQueue.main.async {
      NotificationCenter.default.post(name: .stopScreenCapture, object: nil)
    }
  }
}

/// ì‹¤ì œ ì¹´ë©”ë¼ ë¯¸ë¦¬ë³´ê¸°ë¥¼ ë‹´ë‹¹í•˜ëŠ” UIView
final class CameraPreviewUIView: UIView {

  // MARK: - Properties

  /// AVFoundation ì¹´ë©”ë¼ ë¯¸ë¦¬ë³´ê¸° ë ˆì´ì–´
  private var previewLayer: AVCaptureVideoPreviewLayer?

  /// HaishinKit ë¯¸ë¦¬ë³´ê¸° ë ˆì´ì–´ (ìŠ¤íŠ¸ë¦¬ë° ì¤‘ì¼ ë•Œ ì‚¬ìš©)
  private var hkPreviewLayer: UIView?

  /// ë¹„ë””ì˜¤ ì¶œë ¥ (í†µê³„ ëª©ì )
  private var videoOutput: AVCaptureVideoDataOutput?
  private let videoOutputQueue = DispatchQueue(
    label: "CameraPreviewView.VideoOutput", qos: .userInteractive)

  /// í˜„ì¬ ìº¡ì²˜ ì„¸ì…˜
  var captureSession: AVCaptureSession? {
    didSet {
      // ì²˜ìŒ ì„¤ì •ë  ë•Œë§Œ í”„ë¦¬ë·° ë ˆì´ì–´ ìƒì„±
      if oldValue == nil && captureSession != nil {
        logInfo("ì´ˆê¸° ìº¡ì²˜ ì„¸ì…˜ ì„¤ì • - í”„ë¦¬ë·° ë ˆì´ì–´ ìƒì„±", category: .camera)
        updatePreviewLayer()
      } else if oldValue !== captureSession {
        logInfo("ìº¡ì²˜ ì„¸ì…˜ ë³€ê²½ ê°ì§€ - í”„ë¦¬ë·° ë ˆì´ì–´ ì—…ë°ì´íŠ¸", category: .camera)
        updatePreviewLayer()
      }
    }
  }

  /// HaishinKit ë§¤ë‹ˆì € (ìŠ¤íŠ¸ë¦¬ë° ìƒíƒœ í™•ì¸ìš©)
  var haishinKitManager: HaishinKitManager? {
    didSet {
      updateStreamingStatus()
      setupStatusMonitoring()
    }
  }

  /// ìŠ¤íŠ¸ë¦¬ë° ìƒíƒœ
  private var isStreaming: Bool = false {
    didSet {
      updateStreamingStatusView()
    }
  }

  /// ìŠ¤íŠ¸ë¦¬ë° ìƒíƒœ ëª¨ë‹ˆí„°ë§ íƒ€ì´ë¨¸
  private var statusMonitorTimer: Timer?

  /// ì¹´ë©”ë¼ ì»¨íŠ¸ë¡¤ ì˜¤ë²„ë ˆì´
  private lazy var controlOverlay: CameraControlOverlay = {
    let overlay = CameraControlOverlay()
    overlay.delegate = self
    overlay.translatesAutoresizingMaskIntoConstraints = false
    overlay.backgroundColor = UIColor.clear
    return overlay
  }()

  /// ìŠ¤íŠ¸ë¦¬ë° ìƒíƒœ í‘œì‹œ
  private lazy var streamingStatusView: StreamingStatusView = {
    let statusView = StreamingStatusView()
    statusView.translatesAutoresizingMaskIntoConstraints = false
    statusView.isHidden = true
    return statusView
  }()

  /// í™”ë©´ ìº¡ì²˜ìš© íƒ€ì´ë¨¸
  private var screenCaptureTimer: Timer?

  /// í™”ë©´ ìº¡ì²˜ ìƒíƒœ
  private var isScreenCapturing: Bool = false
  
  /// ìµœê·¼ ì¹´ë©”ë¼ í”„ë ˆì„ (í™”ë©´ ìº¡ì²˜ìš©)
  private var latestCameraFrame: CVPixelBuffer?
  private let frameProcessingQueue = DispatchQueue(label: "CameraFrameProcessing", qos: .userInteractive)
  
  /// í”„ë ˆì„ ì¹´ìš´í„° (í†µê³„ ì¶œë ¥ìš©)
  private var frameCounter = 0

  // MARK: - Initialization

  override init(frame: CGRect) {
    super.init(frame: frame)
    setupView()
  }

  required init?(coder: NSCoder) {
    super.init(coder: coder)
    setupView()
  }

  // MARK: - View Setup

  private func setupView() {
    backgroundColor = .black

    // ì»¨íŠ¸ë¡¤ ì˜¤ë²„ë ˆì´ë§Œ ì¶”ê°€ (StreamingStatusViewëŠ” ì¤‘ë³µë˜ë¯€ë¡œ ì œê±°)
    addSubview(controlOverlay)
    // addSubview(streamingStatusView) // ì¤‘ë³µ ì œê±°

    setupConstraints()
    setupGestureRecognizers()
    setupNotifications()
  }

  private func setupNotifications() {
    // í™”ë©´ ìº¡ì²˜ ì œì–´ notification êµ¬ë…
    NotificationCenter.default.addObserver(
      self,
      selector: #selector(handleStartScreenCapture),
      name: NSNotification.Name("startScreenCapture"),
      object: nil
    )

    NotificationCenter.default.addObserver(
      self,
      selector: #selector(handleStopScreenCapture),
      name: NSNotification.Name("stopScreenCapture"),
      object: nil
    )


  }

  @objc private func handleStartScreenCapture() {
    logDebug("í™”ë©´ ìº¡ì²˜ ì‹œì‘ notification ìˆ˜ì‹ ", category: .streaming)
    startScreenCapture()
  }

  @objc private func handleStopScreenCapture() {
    logDebug("í™”ë©´ ìº¡ì²˜ ì¤‘ì§€ notification ìˆ˜ì‹ ", category: .streaming)
    stopScreenCapture()
  }

  private func setupConstraints() {
    NSLayoutConstraint.activate([
      // ì»¨íŠ¸ë¡¤ ì˜¤ë²„ë ˆì´
      controlOverlay.leadingAnchor.constraint(equalTo: leadingAnchor),
      controlOverlay.trailingAnchor.constraint(equalTo: trailingAnchor),
      controlOverlay.topAnchor.constraint(equalTo: topAnchor),
      controlOverlay.bottomAnchor.constraint(equalTo: bottomAnchor),

      // ìŠ¤íŠ¸ë¦¬ë° ìƒíƒœ í‘œì‹œ ì œì•½ ì œê±° (ì¤‘ë³µ ë°©ì§€)
    ])
  }

  private func setupGestureRecognizers() {
    // í¬ì»¤ìŠ¤ íƒ­ ì œìŠ¤ì²˜
    let focusTapGesture = UITapGestureRecognizer(
      target: self, action: #selector(handleFocusTap(_:)))
    addGestureRecognizer(focusTapGesture)

    // ë…¸ì¶œ ì¡°ì ˆ ë”ë¸”íƒ­ ì œìŠ¤ì²˜
    let exposureDoubleTapGesture = UITapGestureRecognizer(
      target: self, action: #selector(handleExposureDoubleTap(_:)))
    exposureDoubleTapGesture.numberOfTapsRequired = 2
    addGestureRecognizer(exposureDoubleTapGesture)

    focusTapGesture.require(toFail: exposureDoubleTapGesture)

    // ì¤Œ í•€ì¹˜ ì œìŠ¤ì²˜
    let zoomPinchGesture = UIPinchGestureRecognizer(
      target: self, action: #selector(handleZoomPinch(_:)))
    addGestureRecognizer(zoomPinchGesture)
  }

  // MARK: - Preview Layer Management

  private func updatePreviewLayer() {
    // ê¸°ì¡´ ë ˆì´ì–´ ì œê±°
    previewLayer?.removeFromSuperlayer()
    hkPreviewLayer?.removeFromSuperview()

    guard let session = captureSession else { return }

    // í•­ìƒ AVFoundation í”„ë¦¬ë·° ì‚¬ìš© (ì•ˆì •ì„± í–¥ìƒ)
    setupAVFoundationPreview(with: session)

    // ìŠ¤íŠ¸ë¦¬ë° ì¤‘ì´ë©´ ì¶”ê°€ í‘œì‹œ
    if isStreaming {
      addStreamingIndicator()
    }
  }

  /// ìŠ¤íŠ¸ë¦¬ë° í‘œì‹œë§Œ ì¶”ê°€ (í”„ë¦¬ë·° ë ˆì´ì–´ëŠ” ê±´ë“œë¦¬ì§€ ì•ŠìŒ)
  private func addStreamingIndicatorOnly() {
    // ê¸°ì¡´ ìŠ¤íŠ¸ë¦¬ë° í‘œì‹œ ì œê±°
    removeStreamingIndicator()

    logDebug("ìŠ¤íŠ¸ë¦¬ë° í‘œì‹œ ì¶”ê°€", category: .streaming)

    let streamingOverlay = UIView(frame: bounds)
    streamingOverlay.backgroundColor = UIColor.clear
    streamingOverlay.tag = 9999  // ì‹ë³„ìš© íƒœê·¸

    let streamingIndicator = UIView()
    streamingIndicator.backgroundColor = UIColor.red.withAlphaComponent(0.9)
    streamingIndicator.layer.cornerRadius = 12
    streamingIndicator.translatesAutoresizingMaskIntoConstraints = false

    let liveLabel = UILabel()
    liveLabel.text = "ğŸ”´ LIVE"
    liveLabel.textColor = .white
    liveLabel.font = UIFont.boldSystemFont(ofSize: 14)
    liveLabel.translatesAutoresizingMaskIntoConstraints = false

    streamingIndicator.addSubview(liveLabel)
    streamingOverlay.addSubview(streamingIndicator)

    NSLayoutConstraint.activate([
      streamingIndicator.topAnchor.constraint(
        equalTo: streamingOverlay.safeAreaLayoutGuide.topAnchor, constant: 20),
      streamingIndicator.leadingAnchor.constraint(
        equalTo: streamingOverlay.leadingAnchor, constant: 20),
      streamingIndicator.widthAnchor.constraint(equalToConstant: 80),
      streamingIndicator.heightAnchor.constraint(equalToConstant: 32),

      liveLabel.centerXAnchor.constraint(equalTo: streamingIndicator.centerXAnchor),
      liveLabel.centerYAnchor.constraint(equalTo: streamingIndicator.centerYAnchor),
    ])

    addSubview(streamingOverlay)
    hkPreviewLayer = streamingOverlay
  }

  /// ìŠ¤íŠ¸ë¦¬ë° í‘œì‹œ ì œê±°
  private func removeStreamingIndicator() {
    // íƒœê·¸ë¡œ ìŠ¤íŠ¸ë¦¬ë° í‘œì‹œ ì°¾ì•„ì„œ ì œê±°
    if let streamingOverlay = subviews.first(where: { $0.tag == 9999 }) {
      streamingOverlay.removeFromSuperview()
      logDebug("ìŠ¤íŠ¸ë¦¬ë° í‘œì‹œ ì œê±°", category: .streaming)
    }
    hkPreviewLayer = nil
  }

  /// ìŠ¤íŠ¸ë¦¬ë° í‘œì‹œ ì¶”ê°€ (ë ˆì´ì•„ì›ƒìš©)
  private func addStreamingIndicator() {
    addStreamingIndicatorOnly()
  }

  /// í”„ë¦¬ë·° ë ˆì´ì–´ê°€ í™œì„± ìƒíƒœì¸ì§€ í™•ì¸í•˜ê³  í•„ìš”ì‹œ ë³µêµ¬
  private func ensurePreviewLayerActive() {
    guard let session = captureSession else {
      logError("ìº¡ì²˜ ì„¸ì…˜ì´ ì—†ì–´ í”„ë¦¬ë·° ë³´í˜¸ ë¶ˆê°€", category: .camera)
      return
    }

    // í”„ë¦¬ë·° ë ˆì´ì–´ê°€ ì—†ê±°ë‚˜ ì„¸ì…˜ì´ ë‹¤ë¥´ë©´ ë³µêµ¬
    if previewLayer == nil || previewLayer?.session !== session {
      logInfo("í”„ë¦¬ë·° ë ˆì´ì–´ ë³µêµ¬ í•„ìš” - ì¬ìƒì„±", category: .camera)
      setupAVFoundationPreview(with: session)
    } else if let layer = previewLayer {
      // í”„ë¦¬ë·° ë ˆì´ì–´ê°€ ìŠˆí¼ë ˆì´ì–´ì—ì„œ ì œê±°ë˜ì—ˆìœ¼ë©´ ë‹¤ì‹œ ì¶”ê°€
      if layer.superlayer == nil {
        logInfo("í”„ë¦¬ë·° ë ˆì´ì–´ ë‹¤ì‹œ ì¶”ê°€", category: .camera)
        self.layer.insertSublayer(layer, at: 0)
      }

      // í”„ë ˆì„ ì—…ë°ì´íŠ¸
      layer.frame = bounds
    }

    logDebug("í”„ë¦¬ë·° ë ˆì´ì–´ ë³´í˜¸ ì™„ë£Œ", category: .camera)
  }

  /// ë¹„ë””ì˜¤ í”„ë ˆì„ ëª¨ë‹ˆí„°ë§ ì„¤ì • (í†µê³„ ëª©ì )
  private func setupVideoMonitoring(with session: AVCaptureSession) {
            // print("ğŸ“¹ [CameraPreview] ë¹„ë””ì˜¤ í”„ë ˆì„ ëª¨ë‹ˆí„°ë§ ì„¤ì •") // ë°˜ë³µì ì¸ ë¡œê·¸ ë¹„í™œì„±í™”

    // ê¸°ì¡´ ë¹„ë””ì˜¤ ì¶œë ¥ ì œê±°
    if let existingOutput = videoOutput {
      session.removeOutput(existingOutput)
    }

    // ìƒˆë¡œìš´ ë¹„ë””ì˜¤ ì¶œë ¥ ìƒì„± (í†µê³„ ëª©ì )
    let newVideoOutput = AVCaptureVideoDataOutput()
    newVideoOutput.setSampleBufferDelegate(self, queue: videoOutputQueue)

    // ë¹„ë””ì˜¤ ì„¤ì • (ê°€ë²¼ìš´ ì²˜ë¦¬ìš©)
    newVideoOutput.videoSettings = [
      kCVPixelBufferPixelFormatTypeKey as String: kCVPixelFormatType_32BGRA
    ]

    // í”„ë ˆì„ ë“œë¡­ í—ˆìš© (ì„±ëŠ¥ ìµœì í™”)
    newVideoOutput.alwaysDiscardsLateVideoFrames = true

    // ì„¸ì…˜ì— ì¶”ê°€
    if session.canAddOutput(newVideoOutput) {
      session.addOutput(newVideoOutput)
      videoOutput = newVideoOutput
                  // print("âœ… [CameraPreview] ë¹„ë””ì˜¤ í”„ë ˆì„ ëª¨ë‹ˆí„°ë§ ì„¤ì • ì™„ë£Œ") // ë°˜ë³µì ì¸ ë¡œê·¸ ë¹„í™œì„±í™”
        } else {
            logError("ë¹„ë””ì˜¤ í”„ë ˆì„ ëª¨ë‹ˆí„°ë§ ì„¤ì • ì‹¤íŒ¨", category: .camera)
    }
  }

  /// ë¹„ë””ì˜¤ í”„ë ˆì„ ëª¨ë‹ˆí„°ë§ í•´ì œ
  private func removeVideoMonitoring() {
    guard let session = captureSession, let output = videoOutput else { return }

            // print("ğŸ“¹ [CameraPreview] ë¹„ë””ì˜¤ í”„ë ˆì„ ëª¨ë‹ˆí„°ë§ í•´ì œ") // ë°˜ë³µì ì¸ ë¡œê·¸ ë¹„í™œì„±í™”
    session.removeOutput(output)
    videoOutput = nil
  }

  /// ìŠ¤íŠ¸ë¦¬ë° ìƒíƒœ ì—…ë°ì´íŠ¸ (ê°œì„ ëœ ë²„ì „)
  private func updateStreamingStatus() {
    guard let manager = haishinKitManager else {
      isStreaming = false
      // StreamingStatusViewëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (ì¤‘ë³µ ë°©ì§€)
      return
    }

    // ìŠ¤íŠ¸ë¦¬ë° ìƒíƒœì™€ ì—°ê²° ìƒíƒœ ëª¨ë‘ í™•ì¸
    let newStreamingState = manager.isStreaming
    let connectionStatus = manager.connectionStatus
    let currentStatus = manager.currentStatus

    if isStreaming != newStreamingState {
      isStreaming = newStreamingState

      // ìƒíƒœ ë³€í™”ë¥¼ ë¡œê¹…
      if isStreaming {
        logInfo("ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ë¨ - ìŠ¤íŠ¸ë¦¬ë° í‘œì‹œ ì¶”ê°€ ë° í”„ë¦¬ë·° ë³´í˜¸", category: .streaming)

        // ìŠ¤íŠ¸ë¦¬ë° í‘œì‹œ ì¶”ê°€ ë° ë¹„ë””ì˜¤ ëª¨ë‹ˆí„°ë§ ì„¤ì •
        DispatchQueue.main.async { [weak self] in
          self?.addStreamingIndicatorOnly()
          // í”„ë¦¬ë·° ë ˆì´ì–´ê°€ í™œì„± ìƒíƒœì¸ì§€ í™•ì¸í•˜ê³  í•„ìš”ì‹œ ë³µêµ¬
          self?.ensurePreviewLayerActive()
          // ë¹„ë””ì˜¤ í”„ë ˆì„ ëª¨ë‹ˆí„°ë§ ì„¤ì • (í†µê³„ ëª©ì )
          if let session = self?.captureSession {
            self?.setupVideoMonitoring(with: session)
          }
        }
      } else {
        logInfo("ìŠ¤íŠ¸ë¦¬ë° ì¢…ë£Œë¨ - ìŠ¤íŠ¸ë¦¬ë° í‘œì‹œ ì œê±°", category: .streaming)

        // ìŠ¤íŠ¸ë¦¬ë° í‘œì‹œ ì œê±° ë° ë¹„ë””ì˜¤ ëª¨ë‹ˆí„°ë§ í•´ì œ
        DispatchQueue.main.async { [weak self] in
          self?.removeStreamingIndicator()
          // ë¹„ë””ì˜¤ í”„ë ˆì„ ëª¨ë‹ˆí„°ë§ í•´ì œ
          self?.removeVideoMonitoring()
          // í”„ë¦¬ë·° ë ˆì´ì–´ ë³µêµ¬
          self?.ensurePreviewLayerActive()
        }
      }
    }

    // ì—°ê²° ìƒíƒœì— ë”°ë¥¸ ìƒì„¸ UI ì—…ë°ì´íŠ¸
    DispatchQueue.main.async { [weak self] in
      self?.updateDetailedStreamingStatus(
        isStreaming: newStreamingState,
        connectionStatus: connectionStatus,
        status: currentStatus
      )
    }
  }

  /// ìƒì„¸ ìŠ¤íŠ¸ë¦¬ë° ìƒíƒœ UI ì—…ë°ì´íŠ¸ (ë¹„í™œì„±í™” - ì¤‘ë³µ ë°©ì§€)
  private func updateDetailedStreamingStatus(
    isStreaming: Bool,
    connectionStatus: String,
    status: LiveStreamStatus
  ) {
    // StreamingStatusView ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (ì¤‘ë³µ ë°©ì§€)
    // ì‘ì€ ë¼ì´ë¸Œ í‘œì‹œë§Œ ì‚¬ìš©
    //        print("ğŸ“Š [CameraPreview] ìƒì„¸ ìƒíƒœ ì—…ë°ì´íŠ¸ ê±´ë„ˆëœ€ (ì¤‘ë³µ ë°©ì§€)")
  }

  /// ìŠ¤íŠ¸ë¦¬ë° ìƒíƒœ í‘œì‹œ ë·° ì—…ë°ì´íŠ¸ (ë¹„í™œì„±í™” - ì¤‘ë³µ ë°©ì§€)
  private func updateStreamingStatusView() {
    // StreamingStatusView ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (ì¤‘ë³µ ë°©ì§€)
    // ì‘ì€ ë¼ì´ë¸Œ í‘œì‹œë§Œ ì‚¬ìš©
    logDebug("ìŠ¤íŠ¸ë¦¬ë° ìƒíƒœ ë·° ì—…ë°ì´íŠ¸ ê±´ë„ˆëœ€ (ì¤‘ë³µ ë°©ì§€)", category: .streaming)
  }

  /// ìŠ¤íŠ¸ë¦¬ë° ìƒíƒœ ëª¨ë‹ˆí„°ë§ ì„¤ì •
  private func setupStatusMonitoring() {
    // ê¸°ì¡´ íƒ€ì´ë¨¸ ì •ë¦¬
    statusMonitorTimer?.invalidate()

    // ìƒˆ íƒ€ì´ë¨¸ ì„¤ì • (1ì´ˆë§ˆë‹¤ ìƒíƒœ í™•ì¸)
    statusMonitorTimer = Timer.scheduledTimer(withTimeInterval: 1.0, repeats: true) {
      [weak self] _ in
      self?.updateStreamingStatus()
    }
  }

  /// ì •ë¦¬ ì‘ì—…
  deinit {
    statusMonitorTimer?.invalidate()
  }

  /// í”„ë¦¬ë·° ë ˆì´ì–´ ê°•ì œ ìƒˆë¡œê³ ì¹¨ (ìŠ¤íŠ¸ë¦¬ë° ìƒíƒœ ë³€í™” ì‹œ)
  func refreshPreviewLayer() {
    logInfo("í”„ë¦¬ë·° ë ˆì´ì–´ ìƒˆë¡œê³ ì¹¨ ì‹œì‘ (ìŠ¤íŠ¸ë¦¬ë°: \(isStreaming))", category: .camera)

    guard let session = captureSession else {
      logError("ìº¡ì²˜ ì„¸ì…˜ì´ ì—†ì–´ ìƒˆë¡œê³ ì¹¨ ì‹¤íŒ¨", category: .camera)
      return
    }

    // ê¸°ì¡´ í”„ë¦¬ë·° ë ˆì´ì–´ ì™„ì „ ì œê±°
    previewLayer?.removeFromSuperlayer()
    previewLayer = nil
    hkPreviewLayer?.removeFromSuperview()
    hkPreviewLayer = nil

    // ì ì‹œ ëŒ€ê¸° í›„ ìƒíƒœì— ë§ëŠ” í”„ë¦¬ë·° ë ˆì´ì–´ ìƒì„±
    DispatchQueue.main.asyncAfter(deadline: .now() + 0.1) { [weak self] in
      guard let self = self else { return }

      logInfo("AVFoundation í”„ë¦¬ë·° ì„¤ì •", category: .camera)
      self.setupAVFoundationPreview(with: session)

      if self.isStreaming {
        logInfo("ìŠ¤íŠ¸ë¦¬ë° í‘œì‹œ ì¶”ê°€", category: .streaming)
        self.addStreamingIndicator()
      }

      logInfo("í”„ë¦¬ë·° ë ˆì´ì–´ ìƒˆë¡œê³ ì¹¨ ì™„ë£Œ", category: .camera)
    }
  }

  private func setupAVFoundationPreview(with session: AVCaptureSession) {
    logInfo("AVFoundation í”„ë¦¬ë·° ë ˆì´ì–´ ì„¤ì • ì¤‘...", category: .camera)

    let newPreviewLayer = AVCaptureVideoPreviewLayer(session: session)
    
    // 16:9 ë¹„ìœ¨ ê³„ì‚° ë° ì ìš©
    let aspectRatio: CGFloat = 16.0 / 9.0
    let viewBounds = bounds
    
    // 16:9 ë¹„ìœ¨ì— ë§ëŠ” í”„ë ˆì„ ê³„ì‚°
    let previewFrame: CGRect
    if viewBounds.width / viewBounds.height > aspectRatio {
      // ì„¸ë¡œê°€ ê¸°ì¤€: ë†’ì´ì— ë§ì¶°ì„œ ë„ˆë¹„ ê³„ì‚°
      let width = viewBounds.height * aspectRatio
      let offsetX = (viewBounds.width - width) / 2
      previewFrame = CGRect(x: offsetX, y: 0, width: width, height: viewBounds.height)
    } else {
      // ê°€ë¡œê°€ ê¸°ì¤€: ë„ˆë¹„ì— ë§ì¶°ì„œ ë†’ì´ ê³„ì‚°
      let height = viewBounds.width / aspectRatio
      let offsetY = (viewBounds.height - height) / 2
      previewFrame = CGRect(x: 0, y: offsetY, width: viewBounds.width, height: height)
    }
    
    newPreviewLayer.frame = previewFrame
    
    // ì‹¤ì œ ì†¡ì¶œ ì˜ì—­ê³¼ ì¼ì¹˜: resizeAspectFill ì‚¬ìš©
    // ì¹´ë©”ë¼ ì´ë¯¸ì§€ê°€ í”„ë ˆì„ì„ ì™„ì „íˆ ì±„ìš°ë„ë¡ ì„¤ì •
    newPreviewLayer.videoGravity = .resizeAspectFill

    if #available(iOS 17.0, *) {
      newPreviewLayer.connection?.videoRotationAngle = 0
    } else {
      newPreviewLayer.connection?.videoOrientation = .portrait
    }

    layer.insertSublayer(newPreviewLayer, at: 0)
    previewLayer = newPreviewLayer

    logInfo("AVFoundation í”„ë¦¬ë·° ë ˆì´ì–´ ì„¤ì • ì™„ë£Œ", category: .camera)
    logDebug("16:9 ë¹„ìœ¨ í”„ë ˆì„: \(previewFrame)", category: .camera)
    logDebug("videoGravity: resizeAspectFill (ì†¡ì¶œ ì˜ì—­ê³¼ ì¼ì¹˜)", category: .camera)
  }

  override func layoutSubviews() {
    super.layoutSubviews()

    // í”„ë¦¬ë·° ë ˆì´ì–´ í”„ë ˆì„ ì—…ë°ì´íŠ¸ (16:9 ë¹„ìœ¨ ìœ ì§€)
    DispatchQueue.main.async { [weak self] in
      guard let self = self else { return }
      
      // 16:9 ë¹„ìœ¨ ê³„ì‚°
      let aspectRatio: CGFloat = 16.0 / 9.0
      let viewBounds = self.bounds
      
      // 16:9 ë¹„ìœ¨ì— ë§ëŠ” í”„ë ˆì„ ì¬ê³„ì‚°
      let previewFrame: CGRect
      if viewBounds.width / viewBounds.height > aspectRatio {
        // ì„¸ë¡œê°€ ê¸°ì¤€: ë†’ì´ì— ë§ì¶°ì„œ ë„ˆë¹„ ê³„ì‚°
        let width = viewBounds.height * aspectRatio
        let offsetX = (viewBounds.width - width) / 2
        previewFrame = CGRect(x: offsetX, y: 0, width: width, height: viewBounds.height)
      } else {
        // ê°€ë¡œê°€ ê¸°ì¤€: ë„ˆë¹„ì— ë§ì¶°ì„œ ë†’ì´ ê³„ì‚°
        let height = viewBounds.width / aspectRatio
        let offsetY = (viewBounds.height - height) / 2
        previewFrame = CGRect(x: 0, y: offsetY, width: viewBounds.width, height: height)
      }
      
      // í”„ë¦¬ë·° ë ˆì´ì–´ í”„ë ˆì„ ì—…ë°ì´íŠ¸ (16:9 ë¹„ìœ¨ ì ìš©)
      self.previewLayer?.frame = previewFrame
      self.hkPreviewLayer?.frame = previewFrame

      // ë ˆì´ì–´ê°€ ì˜¬ë°”ë¥´ê²Œ í‘œì‹œë˜ë„ë¡ ê°•ì œ ë ˆì´ì•„ì›ƒ ì—…ë°ì´íŠ¸
      if let layer = self.previewLayer {
        layer.setNeedsLayout()
        layer.layoutIfNeeded()
      }
      
      logDebug("ë ˆì´ì•„ì›ƒ ì—…ë°ì´íŠ¸ - 16:9 í”„ë ˆì„: \(previewFrame)", category: .camera)
    }
  }

  // MARK: - Gesture Handlers

  @objc private func handleFocusTap(_ gesture: UITapGestureRecognizer) {
    let point = gesture.location(in: self)
    let focusPoint = CGPoint(
      x: point.x / bounds.width,
      y: point.y / bounds.height
    )

    setFocusPoint(focusPoint)
    showFocusIndicator(at: point)
  }

  @objc private func handleExposureDoubleTap(_ gesture: UITapGestureRecognizer) {
    let point = gesture.location(in: self)
    let exposurePoint = CGPoint(
      x: point.x / bounds.width,
      y: point.y / bounds.height
    )

    setExposurePoint(exposurePoint)
    showExposureIndicator(at: point)
  }

  @objc private func handleZoomPinch(_ gesture: UIPinchGestureRecognizer) {
    guard let device = getCurrentCameraDevice() else { return }

    do {
      try device.lockForConfiguration()

      let maxZoom = min(device.activeFormat.videoMaxZoomFactor, 4.0)
      let currentZoom = device.videoZoomFactor
      let newZoom = currentZoom * gesture.scale

      device.videoZoomFactor = max(1.0, min(newZoom, maxZoom))

      device.unlockForConfiguration()
      gesture.scale = 1.0

    } catch {
              logError("Zoom adjustment failed: \(error)", category: .camera)
    }
  }

  // MARK: - Camera Control Methods

  private func setFocusPoint(_ point: CGPoint) {
    guard let device = getCurrentCameraDevice() else { return }

    do {
      try device.lockForConfiguration()

      if device.isFocusPointOfInterestSupported {
        device.focusPointOfInterest = point
        device.focusMode = .autoFocus
      }

      if device.isExposurePointOfInterestSupported {
        device.exposurePointOfInterest = point
        device.exposureMode = .autoExpose
      }

      device.unlockForConfiguration()
    } catch {
              logError("Focus adjustment failed: \(error)", category: .camera)
    }
  }

  private func setExposurePoint(_ point: CGPoint) {
    guard let device = getCurrentCameraDevice() else { return }

    do {
      try device.lockForConfiguration()

      if device.isExposurePointOfInterestSupported {
        device.exposurePointOfInterest = point
        device.exposureMode = .continuousAutoExposure
      }

      device.unlockForConfiguration()
    } catch {
              logError("Exposure adjustment failed: \(error)", category: .camera)
    }
  }

  private func getCurrentCameraDevice() -> AVCaptureDevice? {
    return captureSession?.inputs.compactMap { input in
      (input as? AVCaptureDeviceInput)?.device
    }.first { $0.hasMediaType(.video) }
  }

  // MARK: - Visual Feedback

  private func showFocusIndicator(at point: CGPoint) {
    let indicator = FocusIndicatorView(frame: CGRect(x: 0, y: 0, width: 80, height: 80))
    indicator.center = point
    addSubview(indicator)

    indicator.animate {
      DispatchQueue.main.asyncAfter(deadline: .now() + 1.5) {
        indicator.removeFromSuperview()
      }
    }
  }

  private func showExposureIndicator(at point: CGPoint) {
    let indicator = ExposureIndicatorView(frame: CGRect(x: 0, y: 0, width: 60, height: 60))
    indicator.center = point
    addSubview(indicator)

    indicator.animate {
      DispatchQueue.main.asyncAfter(deadline: .now() + 1.5) {
        indicator.removeFromSuperview()
      }
    }
  }

  // MARK: - Streaming State Management

  func updateStreamingState(_ isStreaming: Bool) {
    self.isStreaming = isStreaming
    streamingStatusView.isHidden = !isStreaming
    updatePreviewLayer()
  }

  func updateStreamingStats(_ stats: StreamStats) {
    streamingStatusView.updateStats(stats)
  }

  // MARK: - Screen Capture for Streaming

  /// CameraPreviewUIViewì˜ í™”ë©´ ìº¡ì²˜ ì†¡ì¶œ ê¸°ëŠ¥
  /// 
  /// ì´ ê¸°ëŠ¥ì€ ë‹¤ìŒê³¼ ê°™ì€ ê³¼ì •ìœ¼ë¡œ ë™ì‘í•©ë‹ˆë‹¤:
  /// 1. ì‹¤ì‹œê°„ ì¹´ë©”ë¼ í”„ë ˆì„ì„ CVPixelBufferë¡œ ìº¡ì²˜
  /// 2. UI ì˜¤ë²„ë ˆì´(ë²„íŠ¼, ë¼ë²¨, ì›Œí„°ë§ˆí¬ ë“±)ë¥¼ ë³„ë„ë¡œ ë Œë”ë§
  /// 3. ì¹´ë©”ë¼ í”„ë ˆì„ê³¼ UIë¥¼ í•©ì„±í•˜ì—¬ ìµœì¢… ì´ë¯¸ì§€ ìƒì„±
  /// 4. 30fpsë¡œ HaishinKitì„ í†µí•´ ìŠ¤íŠ¸ë¦¬ë° ì„œë²„ì— ì „ì†¡
  ///
  /// **ì£¼ì˜ì‚¬í•­:**
  /// - ì¹´ë©”ë¼ í”„ë ˆì„ì´ ì—†ì„ ê²½ìš° UIë§Œ ìº¡ì²˜ë©ë‹ˆë‹¤
  /// - AVCaptureVideoPreviewLayerëŠ” í•˜ë“œì›¨ì–´ ê°€ì† ë ˆì´ì–´ì´ë¯€ë¡œ ì§ì ‘ ìº¡ì²˜ê°€ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤
  /// - ë”°ë¼ì„œ AVCaptureVideoDataOutputì—ì„œ ë°›ì€ ì‹¤ì œ ì¹´ë©”ë¼ í”„ë ˆì„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤

  /// í™”ë©´ ìº¡ì²˜ ì†¡ì¶œ ì‹œì‘
  /// 
  /// 30fps íƒ€ì´ë¨¸ë¥¼ ì‹œì‘í•˜ì—¬ ì§€ì†ì ìœ¼ë¡œ í™”ë©´ì„ ìº¡ì²˜í•˜ê³  ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤.
  /// ì¹´ë©”ë¼ í”„ë ˆì„ê³¼ UIë¥¼ í•©ì„±í•œ ì™„ì „í•œ í™”ë©´ì´ ì†¡ì¶œë©ë‹ˆë‹¤.
  func startScreenCapture() {
    guard !isScreenCapturing else { 
      logWarning("ì´ë¯¸ í™”ë©´ ìº¡ì²˜ê°€ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤", category: .streaming)
      return 
    }

    isScreenCapturing = true
            logInfo("í™”ë©´ ìº¡ì²˜ ì†¡ì¶œ ì‹œì‘", category: .streaming)

    // **ì„±ëŠ¥ ìµœì í™”**: 30fps â†’ 25fpsë¡œ ë‚®ì¶°ì„œ CPU ë¶€í•˜ ê°ì†Œ
    // 25fpsëŠ” ì—¬ì „íˆ ë¶€ë“œëŸ¬ìš´ ìŠ¤íŠ¸ë¦¬ë°ì„ ì œê³µí•˜ë©´ì„œ ì‹œìŠ¤í…œ ë¶€í•˜ë¥¼ ì¤„ì„
    screenCaptureTimer = Timer.scheduledTimer(withTimeInterval: 1.0 / 25.0, repeats: true) {
      [weak self] _ in
      self?.captureCurrentFrame()
    }
    
            // print("âœ… [CameraPreview] í™”ë©´ ìº¡ì²˜ íƒ€ì´ë¨¸ ì‹œì‘ë¨ (25fps - ì„±ëŠ¥ ìµœì í™”)") // ë°˜ë³µì ì¸ ë¡œê·¸ ë¹„í™œì„±í™”
  }
  
  /// í™”ë©´ ìº¡ì²˜ ì†¡ì¶œ ì¤‘ì§€
  /// 
  /// íƒ€ì´ë¨¸ë¥¼ ì¤‘ì§€í•˜ê³  ìº¡ì²˜ëœ í”„ë ˆì„ ë°ì´í„°ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤.
  func stopScreenCapture() {
    guard isScreenCapturing else { 
      logWarning("í™”ë©´ ìº¡ì²˜ê°€ ì‹¤í–‰ ì¤‘ì´ì§€ ì•ŠìŠµë‹ˆë‹¤", category: .streaming)
      return 
    }

    isScreenCapturing = false
    screenCaptureTimer?.invalidate()
    screenCaptureTimer = nil
    
    // ë©”ëª¨ë¦¬ ì •ë¦¬: ìµœê·¼ ìº¡ì²˜ëœ ì¹´ë©”ë¼ í”„ë ˆì„ ì œê±°
    frameProcessingQueue.async { [weak self] in
      self?.latestCameraFrame = nil
    }
    
    logInfo("í™”ë©´ ìº¡ì²˜ ì†¡ì¶œ ì¤‘ì§€ ë° ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ", category: .streaming)
  }

  /// í˜„ì¬ í”„ë ˆì„ ìº¡ì²˜ ë° HaishinKit ì „ì†¡
  /// 
  /// ì´ ë©”ì„œë“œëŠ” 30fps íƒ€ì´ë¨¸ì— ì˜í•´ í˜¸ì¶œë˜ë©°, ë‹¤ìŒ ë‹¨ê³„ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤:
  /// 1. ë©”ì¸ ìŠ¤ë ˆë“œì—ì„œ UI ë Œë”ë§ ìˆ˜í–‰
  /// 2. ì¹´ë©”ë¼ í”„ë ˆì„ê³¼ UIë¥¼ í•©ì„±í•˜ì—¬ ìµœì¢… ì´ë¯¸ì§€ ìƒì„±
  /// 3. UIImageë¥¼ CVPixelBufferë¡œ ë³€í™˜
  /// 4. HaishinKitì„ í†µí•´ ìŠ¤íŠ¸ë¦¬ë° ì„œë²„ì— ì „ì†¡
  private func captureCurrentFrame() {
    DispatchQueue.main.async { [weak self] in
      guard let self = self else { return }

      // í™”ë©´ ìº¡ì²˜ ìƒíƒœ ì¬í™•ì¸ (íƒ€ì´ë¨¸ ì§€ì—°ìœ¼ë¡œ ì¸í•œ ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€)
      guard self.isScreenCapturing else { return }

      // Step 1: í˜„ì¬ í™”ë©´ì„ ì´ë¯¸ì§€ë¡œ ë Œë”ë§ (ì¹´ë©”ë¼ í”„ë ˆì„ + UI í•©ì„±)
      guard let capturedImage = self.renderToImage() else {
                    // print("âŒ [í™”ë©´ìº¡ì²˜] UIImage ë Œë”ë§ ì‹¤íŒ¨ - í”„ë ˆì„ ìŠ¤í‚µ") // ë°˜ë³µì ì¸ ë¡œê·¸ ë¹„í™œì„±í™”
        return
      }
      
      // ì„±ëŠ¥ ìµœì í™”: í”„ë ˆì„ë³„ ìƒì„¸ ë¡œê·¸ ì œê±° (CPU ë¶€í•˜ ê°ì†Œ)
      // print("âœ… [í™”ë©´ìº¡ì²˜] í™”ë©´ ë Œë”ë§ ì„±ê³µ: \(capturedImage.size)")

      // Step 2: UIImageë¥¼ CVPixelBufferë¡œ ë³€í™˜ (HaishinKit í˜¸í™˜ í¬ë§·)
      guard let pixelBuffer = capturedImage.toCVPixelBuffer() else {
                    // print("âŒ [í™”ë©´ìº¡ì²˜] CVPixelBuffer ë³€í™˜ ì‹¤íŒ¨ - í”„ë ˆì„ ìŠ¤í‚µ") // ë°˜ë³µì ì¸ ë¡œê·¸ ë¹„í™œì„±í™”
        return
      }
      
      // ì„±ëŠ¥ ìµœì í™”: ë³€í™˜ ì„±ê³µ ë¡œê·¸ ì œê±°
      // let width = CVPixelBufferGetWidth(pixelBuffer)
      // let height = CVPixelBufferGetHeight(pixelBuffer)
      // print("âœ… [í™”ë©´ìº¡ì²˜] CVPixelBuffer ë³€í™˜ ì„±ê³µ: \(width)x\(height)")

      // Step 3: HaishinKitì„ í†µí•´ ìŠ¤íŠ¸ë¦¬ë° ì„œë²„ì— ì „ì†¡
      self.sendFrameToHaishinKit(pixelBuffer)
    }
  }

  /// UIViewë¥¼ UIImageë¡œ ë Œë”ë§ (ì¹´ë©”ë¼ í”„ë ˆì„ + UI í•©ì„±)
  /// 
  /// ì´ ë©”ì„œë“œëŠ” í™”ë©´ ìº¡ì²˜ì˜ í•µì‹¬ ë¡œì§ì…ë‹ˆë‹¤:
  /// - ì¹´ë©”ë¼ í”„ë ˆì„ì´ ìˆìœ¼ë©´: ì¹´ë©”ë¼ ì˜ìƒ + UI ì˜¤ë²„ë ˆì´ í•©ì„±
  /// - ì¹´ë©”ë¼ í”„ë ˆì„ì´ ì—†ìœ¼ë©´: UIë§Œ ìº¡ì²˜ (ê¸°ë³¸ ë ˆì´ì–´ ë Œë”ë§)
  ///
  /// **ê¸°ìˆ ì  ë°°ê²½:**
  /// AVCaptureVideoPreviewLayerëŠ” í•˜ë“œì›¨ì–´ ê°€ì†ì„ ì‚¬ìš©í•˜ë¯€ë¡œ 
  /// ì¼ë°˜ì ì¸ layer.render() ë°©ì‹ìœ¼ë¡œëŠ” ìº¡ì²˜ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
  /// ë”°ë¼ì„œ AVCaptureVideoDataOutputì—ì„œ ë°›ì€ ì‹¤ì œ í”„ë ˆì„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
  ///
  /// - Returns: ìº¡ì²˜ëœ ìµœì¢… ì´ë¯¸ì§€ (ì¹´ë©”ë¼ + UI í•©ì„±) ë˜ëŠ” nil
  private func renderToImage() -> UIImage? {
    // ì†¡ì¶œìš© ê³ í•´ìƒë„ ë Œë”ë§ ì‚¬ìš© (í•´ìƒë„ ë¬¸ì œ í•´ê²°)
    return renderToImageForStreaming()
  }
  
  /// ì†¡ì¶œìš© ê³ í•´ìƒë„ UI ë Œë”ë§ (í•´ìƒë„ ë¬¸ì œ í•´ê²°)
  /// 
  /// **ê°œì„ ëœ ì „ëµ:**
  /// - 480p ì†¡ì¶œ â†’ ì•½ 1000p(1712x960) ìº¡ì²˜
  /// - 720p ì†¡ì¶œ â†’ ì•½ 1400p(2560x1440) ìº¡ì²˜  
  /// - 1080p ì†¡ì¶œ â†’ ë™ì¼ í•´ìƒë„(1920x1080) ìº¡ì²˜ (ì•ˆì •ì„± ìš°ì„ )
  /// - ì†¡ì¶œ í•´ìƒë„ë³´ë‹¤ 2ë°° ì •ë„ ë†’ì€ í•´ìƒë„ë¡œ ìº¡ì²˜í•˜ì—¬ ê³ í’ˆì§ˆ ìœ ì§€
  /// 
  /// - Returns: ì†¡ì¶œ í•´ìƒë„ì— ë”°ë¼ ìµœì í™”ëœ ê³ í’ˆì§ˆ ì´ë¯¸ì§€
  private func renderToImageForStreaming() -> UIImage? {
    // HaishinKitManagerì—ì„œ í˜„ì¬ ìŠ¤íŠ¸ë¦¬ë° ì„¤ì • ê°€ì ¸ì˜¤ê¸°
    let streamingSize = getOptimalCaptureSize()
    
    logDebug("ì†¡ì¶œìš© UI ë Œë”ë§ ì‹œì‘: \(streamingSize)", category: .performance)
    
    // ìµœê·¼ ì¹´ë©”ë¼ í”„ë ˆì„ì´ ìˆëŠ”ì§€ í™•ì¸
    if let cameraFrame = latestCameraFrame {
      // ì¼€ì´ìŠ¤ 1: ì¹´ë©”ë¼ í”„ë ˆì„ + UI í•©ì„± (ê³ í•´ìƒë„)
              // print("ğŸ¥ [ê³ í•´ìƒë„ ë Œë”ë§] ì¹´ë©”ë¼ í”„ë ˆì„ + UI í•©ì„± ëª¨ë“œ") // ë°˜ë³µì ì¸ ë¡œê·¸ ë¹„í™œì„±í™”
      return renderCameraFrameWithUIForStreaming(cameraFrame: cameraFrame, streamingSize: streamingSize)
    } else {
      // ì¼€ì´ìŠ¤ 2: UIë§Œ ê³ í•´ìƒë„ ìº¡ì²˜ (ì¹´ë©”ë¼ í”„ë ˆì„ ì—†ìŒ)
      logDebug("UIë§Œ ìº¡ì²˜ ëª¨ë“œ (ê³ í•´ìƒë„)", category: .performance)
      return renderUIOnlyForStreaming(streamingSize: streamingSize)
    }
  }
  
  /// ë‹¨ë§ í‘œì‹œìš© ì¼ë°˜ í•´ìƒë„ ë Œë”ë§ (ê¸°ì¡´ ë°©ì‹ ìœ ì§€)
  /// 
  /// ì‚¬ìš©ìê°€ iPadì—ì„œ ë³´ëŠ” í™”ë©´ìš©ìœ¼ë¡œ ê¸°ì¡´ í¬ê¸° ìœ ì§€
  /// - Returns: ë‹¨ë§ í™”ë©´ í¬ê¸°ì˜ ì´ë¯¸ì§€
  private func renderToImageForDisplay() -> UIImage? {
    let size = bounds.size
    guard size.width > 0 && size.height > 0 else { 
      logError("ìœ íš¨í•˜ì§€ ì•Šì€ ë·° í¬ê¸°: \(size)", category: .performance)
      return nil 
    }
    
    logDebug("í‘œì‹œìš© UI ë Œë”ë§: \(size)", category: .performance)
    
    // ìµœê·¼ ì¹´ë©”ë¼ í”„ë ˆì„ì´ ìˆëŠ”ì§€ í™•ì¸
    if let cameraFrame = latestCameraFrame {
      // ì¼€ì´ìŠ¤ 1: ì¹´ë©”ë¼ í”„ë ˆì„ + UI í•©ì„± (ë‹¨ë§ í¬ê¸°)
              // print("ğŸ¥ [ë‹¨ë§ë Œë”ë§] ì¹´ë©”ë¼ í”„ë ˆì„ + UI í•©ì„± ëª¨ë“œ") // ë°˜ë³µì ì¸ ë¡œê·¸ ë¹„í™œì„±í™”
      return renderCameraFrameWithUI(cameraFrame: cameraFrame, viewSize: size)
    } else {
      // ì¼€ì´ìŠ¤ 2: UIë§Œ ìº¡ì²˜ (ë‹¨ë§ í¬ê¸°)
      logDebug("UIë§Œ ìº¡ì²˜ ëª¨ë“œ", category: .performance)
      let renderer = UIGraphicsImageRenderer(bounds: bounds)
      return renderer.image { context in
        layer.render(in: context.cgContext)
      }
    }
  }
  
  /// ì†¡ì¶œìš© ê³ í•´ìƒë„ ì¹´ë©”ë¼ í”„ë ˆì„ê³¼ UI í•©ì„±
  /// 
  /// 1920x1080 í¬ê¸°ë¡œ ê³ í’ˆì§ˆ ë Œë”ë§í•˜ì—¬ ì—…ìŠ¤ì¼€ì¼ë§ìœ¼ë¡œ ì¸í•œ í™”ì§ˆ ì €í•˜ ë°©ì§€
  /// 
  /// - Parameter cameraFrame: ì‹¤ì‹œê°„ ì¹´ë©”ë¼ í”„ë ˆì„ (CVPixelBuffer)
  /// - Parameter streamingSize: ì†¡ì¶œ ëª©í‘œ í•´ìƒë„ (1920x1080)
  /// - Returns: ê³ í•´ìƒë„ í•©ì„± ì´ë¯¸ì§€ ë˜ëŠ” nil
  private func renderCameraFrameWithUIForStreaming(cameraFrame: CVPixelBuffer, streamingSize: CGSize) -> UIImage? {
    
    // Step 1: ì¹´ë©”ë¼ í”„ë ˆì„ì„ UIImageë¡œ ë³€í™˜
    guard let cameraImage = cameraFrame.toUIImage() else {
      logError("ì¹´ë©”ë¼ í”„ë ˆì„ â†’ UIImage ë³€í™˜ ì‹¤íŒ¨", category: .performance)
      return nil
    }
    logDebug("ì¹´ë©”ë¼ ì´ë¯¸ì§€ ë³€í™˜ ì„±ê³µ: \(cameraImage.size)", category: .performance)
    
    // Step 2: UI ì˜¤ë²„ë ˆì´ë¥¼ ê³ í•´ìƒë„ë¡œ ìƒì„± (1:1 â†’ 16:9 ë¹„ìœ¨ ê°•ì œ ë³€í™˜)
    // ë‹¨ë§ í¬ê¸°ì—ì„œ ì†¡ì¶œ í¬ê¸°ë¡œ ìŠ¤ì¼€ì¼ë§ ë¹„ìœ¨ ê³„ì‚°
    let currentSize = bounds.size
    let originalAspectRatio = currentSize.width / currentSize.height
    let targetAspectRatio = streamingSize.width / streamingSize.height
    
    let scaleX = streamingSize.width / currentSize.width
    let scaleY = streamingSize.height / currentSize.height
    let scale = max(scaleX, scaleY) // **Aspect Fill**: í™”ë©´ ê½‰ ì±„ìš°ê¸° (1:1 ë¬¸ì œ í•´ê²°)
    
    logDebug("ë¹„ìœ¨ ë¶„ì„:", category: .performance)
    logDebug("  â€¢ ì›ë³¸ UI: \(currentSize) (ë¹„ìœ¨: \(String(format: "%.2f", originalAspectRatio)))", category: .performance)
    logDebug("  â€¢ ëª©í‘œ ì†¡ì¶œ: \(streamingSize) (ë¹„ìœ¨: \(String(format: "%.2f", targetAspectRatio)))", category: .performance)
    logDebug("  â€¢ Aspect Fill ìŠ¤ì¼€ì¼: \(String(format: "%.2f", scale))x", category: .performance)
    
    // 1:1 ë¹„ìœ¨ ë¬¸ì œ ê°ì§€
    if abs(originalAspectRatio - 1.0) < 0.2 {
      logWarning("1:1 ë¬¸ì œ ê°ì§€ - ì¹´ë©”ë¼+UI í•©ì„±ì—ì„œ ì •ì‚¬ê°í˜• UI ê°ì§€ â†’ Aspect Fill ì ìš©", category: .performance)
    }
    
    let uiRenderer = UIGraphicsImageRenderer(size: streamingSize)
    let uiOverlay = uiRenderer.image { context in
      // Aspect Fill ìŠ¤ì¼€ì¼ë§ìœ¼ë¡œ UI ë Œë”ë§ (í™”ë©´ ê½‰ ì±„ìš°ê¸°)
      context.cgContext.scaleBy(x: scale, y: scale)
      
      // UIê°€ ì˜ë¦´ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì¤‘ì•™ ì •ë ¬
      let scaledSize = CGSize(width: currentSize.width * scale, height: currentSize.height * scale)
      let offsetX = (streamingSize.width - scaledSize.width) / 2.0
      let offsetY = (streamingSize.height - scaledSize.height) / 2.0
      context.cgContext.translateBy(x: offsetX / scale, y: offsetY / scale)
      
      // í”„ë¦¬ë·° ë ˆì´ì–´ë¥¼ ì œì™¸í•œ ëª¨ë“  ì„œë¸Œë·° ë Œë”ë§
      for subview in subviews {
        // AVCaptureVideoPreviewLayerëŠ” ì œì™¸ (ì¹´ë©”ë¼ í”„ë ˆì„ìœ¼ë¡œ ëŒ€ì²´ë¨)
        if !(subview.layer is AVCaptureVideoPreviewLayer) {
          subview.layer.render(in: context.cgContext)
        }
      }
    }
    logDebug("UI ì˜¤ë²„ë ˆì´ ìƒì„± ì™„ë£Œ: \(streamingSize)", category: .performance)
    
    // Step 3: ì¹´ë©”ë¼ ì´ë¯¸ì§€ì™€ UI ì˜¤ë²„ë ˆì´ë¥¼ ê³ í•´ìƒë„ë¡œ í•©ì„±
    let finalRenderer = UIGraphicsImageRenderer(size: streamingSize)
    let compositeImage = finalRenderer.image { context in
      let rect = CGRect(origin: .zero, size: streamingSize)
      
      // 3-1: ì¹´ë©”ë¼ ì´ë¯¸ì§€ë¥¼ UIì™€ ë™ì¼í•œ ë¹„ìœ¨ë¡œ ì—…ìŠ¤ì¼€ì¼ë§
      // ë‹¨ë§ì—ì„œì˜ ì¹´ë©”ë¼ í”„ë¦¬ë·° ì˜ì—­ì„ ê³„ì‚°
      let cameraPreviewRect = calculateCameraPreviewRect(in: currentSize)
      
      // ì¹´ë©”ë¼ í”„ë¦¬ë·° ì˜ì—­ì„ ë™ì¼í•œ ìŠ¤ì¼€ì¼ ë¹„ìœ¨ë¡œ ì—…ìŠ¤ì¼€ì¼ë§
      let scaledCameraRect = CGRect(
        x: cameraPreviewRect.origin.x * scale,
        y: cameraPreviewRect.origin.y * scale,
        width: cameraPreviewRect.size.width * scale,
        height: cameraPreviewRect.size.height * scale
      )
      
      logDebug("ì¹´ë©”ë¼ ì˜ì—­ ìŠ¤ì¼€ì¼ë§: \(cameraPreviewRect) â†’ \(scaledCameraRect)", category: .performance)
      
      // ì¹´ë©”ë¼ ì´ë¯¸ì§€ë¥¼ ìŠ¤ì¼€ì¼ëœ ì˜ì—­ì— ë§ì¶° ê·¸ë¦¬ê¸° (Aspect Fill ë°©ì‹)
      // Aspect Fillë¡œ ê·¸ë ¤ì„œ ì¹´ë©”ë¼ ì´ë¯¸ì§€ê°€ ì˜ë¦¬ì§€ ì•Šë„ë¡ í•¨
      let cameraAspectRatio = cameraImage.size.width / cameraImage.size.height
      let rectAspectRatio = scaledCameraRect.width / scaledCameraRect.height
      
      let drawRect: CGRect
      if cameraAspectRatio > rectAspectRatio {
        // ì¹´ë©”ë¼ê°€ ë” ë„“ìŒ: ë†’ì´ë¥¼ ë§ì¶”ê³  ê°€ë¡œëŠ” ë„˜ì¹¨
        let drawHeight = scaledCameraRect.height
        let drawWidth = drawHeight * cameraAspectRatio
        let offsetX = scaledCameraRect.origin.x + (scaledCameraRect.width - drawWidth) / 2
        drawRect = CGRect(x: offsetX, y: scaledCameraRect.origin.y, width: drawWidth, height: drawHeight)
      } else {
        // ì¹´ë©”ë¼ê°€ ë” ë†’ìŒ: ë„ˆë¹„ë¥¼ ë§ì¶”ê³  ì„¸ë¡œëŠ” ë„˜ì¹¨
        let drawWidth = scaledCameraRect.width
        let drawHeight = drawWidth / cameraAspectRatio
        let offsetY = scaledCameraRect.origin.y + (scaledCameraRect.height - drawHeight) / 2
        drawRect = CGRect(x: scaledCameraRect.origin.x, y: offsetY, width: drawWidth, height: drawHeight)
      }
      
      logDebug("ì¹´ë©”ë¼ ì´ë¯¸ì§€ Aspect Fill ê·¸ë¦¬ê¸°: \(scaledCameraRect) â†’ \(drawRect)", category: .performance)
      cameraImage.draw(in: drawRect)
      
      // 3-2: UI ì˜¤ë²„ë ˆì´ë¥¼ ì „ì²´ í™”ë©´ì— í•©ì„±
      uiOverlay.draw(in: rect, blendMode: .normal, alpha: 1.0)
    }
    
    logDebug("ìµœì¢… ì´ë¯¸ì§€ í•©ì„± ì™„ë£Œ: \(streamingSize)", category: .performance)
    return compositeImage
  }
  
  /// ë‹¨ë§ í™”ë©´ì—ì„œ ì¹´ë©”ë¼ í”„ë¦¬ë·°ê°€ ì°¨ì§€í•˜ëŠ” 16:9 ì˜ì—­ ê³„ì‚°
  /// 
  /// ì‹¤ì œ ì†¡ì¶œë˜ëŠ” 16:9 ë¹„ìœ¨ ì˜ì—­ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
  /// ì´ë¥¼ í†µí•´ í”„ë¦¬ë·°ì™€ ì†¡ì¶œ í™”ë©´ì´ ì •í™•íˆ ì¼ì¹˜í•˜ë„ë¡ í•©ë‹ˆë‹¤.
  /// 
  /// - Parameter containerSize: ì»¨í…Œì´ë„ˆ ë·°ì˜ í¬ê¸° (ë‹¨ë§ í™”ë©´ í¬ê¸°)
  /// - Returns: 16:9 ë¹„ìœ¨ë¡œ ê³„ì‚°ëœ ì¹´ë©”ë¼ í”„ë¦¬ë·° ì˜ì—­
  private func calculateCameraPreviewRect(in containerSize: CGSize) -> CGRect {
    // 16:9 ë¹„ìœ¨ë¡œ ê³ ì •ëœ ì†¡ì¶œ ì˜ì—­ ê³„ì‚°
    let aspectRatio: CGFloat = 16.0 / 9.0
    
    let previewFrame: CGRect
    if containerSize.width / containerSize.height > aspectRatio {
      // ì„¸ë¡œê°€ ê¸°ì¤€: ë†’ì´ì— ë§ì¶°ì„œ ë„ˆë¹„ ê³„ì‚°
      let width = containerSize.height * aspectRatio
      let offsetX = (containerSize.width - width) / 2
      previewFrame = CGRect(x: offsetX, y: 0, width: width, height: containerSize.height)
    } else {
      // ê°€ë¡œê°€ ê¸°ì¤€: ë„ˆë¹„ì— ë§ì¶°ì„œ ë†’ì´ ê³„ì‚°
      let height = containerSize.width / aspectRatio
      let offsetY = (containerSize.height - height) / 2
      previewFrame = CGRect(x: 0, y: offsetY, width: containerSize.width, height: height)
    }
    
    logDebug("16:9 ë¹„ìœ¨ ì†¡ì¶œ ì˜ì—­: \(previewFrame)", category: .camera)
    return previewFrame
  }
  
  /// AVCaptureVideoPreviewLayerì˜ ì‹¤ì œ ë¹„ë””ì˜¤ í‘œì‹œ ì˜ì—­ ê³„ì‚°
  /// 
  /// videoGravity ì„¤ì •ì— ë”°ë¼ ì‹¤ì œë¡œ ë¹„ë””ì˜¤ê°€ í‘œì‹œë˜ëŠ” ì˜ì—­ì„ ì •í™•íˆ ê³„ì‚°í•©ë‹ˆë‹¤.
  /// - resizeAspect: ë¹„ë””ì˜¤ ë¹„ìœ¨ ìœ ì§€, ë ˆì´ì–´ ë‚´ë¶€ì— ë§ì¶¤ (ê²€ì€ ì—¬ë°± ê°€ëŠ¥)
  /// - resizeAspectFill: ë¹„ë””ì˜¤ ë¹„ìœ¨ ìœ ì§€, ë ˆì´ì–´ ì „ì²´ë¥¼ ì±„ì›€ (ì¼ë¶€ ì˜ë¦¼ ê°€ëŠ¥)
  /// - resize: ë¹„ë””ì˜¤ë¥¼ ë ˆì´ì–´ í¬ê¸°ì— ë§ì¶° ëŠ˜ë¦¼ (ë¹„ìœ¨ ì™œê³¡ ê°€ëŠ¥)
  /// 
  /// - Parameter previewLayer: ì¹´ë©”ë¼ í”„ë¦¬ë·° ë ˆì´ì–´
  /// - Returns: ì‹¤ì œ ë¹„ë””ì˜¤ê°€ í‘œì‹œë˜ëŠ” ì˜ì—­
  private func calculateActualVideoRect(previewLayer: AVCaptureVideoPreviewLayer) -> CGRect {
    let layerBounds = previewLayer.bounds
    let videoGravity = previewLayer.videoGravity
    
    // ì¹´ë©”ë¼ ì„¸ì…˜ì—ì„œ ë¹„ë””ì˜¤ ì…ë ¥ì˜ ì‹¤ì œ í•´ìƒë„ ê°€ì ¸ì˜¤ê¸°
    guard let session = previewLayer.session else {
      logWarning("ì„¸ì…˜ ì—†ìŒ, ë ˆì´ì–´ ì „ì²´ ì˜ì—­ ë°˜í™˜: \(layerBounds)", category: .camera)
      return layerBounds
    }
    
    // í˜„ì¬ í™œì„± ë¹„ë””ì˜¤ ì…ë ¥ì˜ í•´ìƒë„ ì°¾ê¸°
    var videoSize: CGSize?
    for input in session.inputs {
      if let deviceInput = input as? AVCaptureDeviceInput,
         deviceInput.device.hasMediaType(.video) {
        let format = deviceInput.device.activeFormat
        let dimensions = CMVideoFormatDescriptionGetDimensions(format.formatDescription)
        videoSize = CGSize(width: Int(dimensions.width), height: Int(dimensions.height))
        break
      }
    }
    
    guard let actualVideoSize = videoSize else {
      logWarning("ë¹„ë””ì˜¤ í¬ê¸° í™•ì¸ ë¶ˆê°€, ë ˆì´ì–´ ì „ì²´ ì˜ì—­ ë°˜í™˜: \(layerBounds)", category: .camera)
      return layerBounds
    }
    
    logDebug("ë¹„ë””ì˜¤ í¬ê¸°: \(actualVideoSize), ë ˆì´ì–´ í¬ê¸°: \(layerBounds.size), ì¤‘ë ¥: \(videoGravity)", category: .camera)
    
    let videoRect: CGRect
    
    switch videoGravity {
    case .resizeAspectFill:
      // Aspect Fill: ë¹„ë””ì˜¤ ë¹„ìœ¨ ìœ ì§€í•˜ë©´ì„œ ë ˆì´ì–´ ì „ì²´ë¥¼ ì±„ì›€ (ì¼ë¶€ ì˜ë¦¼ ê°€ëŠ¥)
      let videoAspectRatio = actualVideoSize.width / actualVideoSize.height
      let layerAspectRatio = layerBounds.width / layerBounds.height
      
      if videoAspectRatio > layerAspectRatio {
        // ë¹„ë””ì˜¤ê°€ ë” ë„“ìŒ: ì„¸ë¡œë¥¼ ë ˆì´ì–´ì— ë§ì¶”ê³  ê°€ë¡œëŠ” ë„˜ì¹¨
        let scaledHeight = layerBounds.height
        let scaledWidth = scaledHeight * videoAspectRatio
        let offsetX = (layerBounds.width - scaledWidth) / 2
        videoRect = CGRect(x: offsetX, y: 0, width: scaledWidth, height: scaledHeight)
      } else {
        // ë¹„ë””ì˜¤ê°€ ë” ë†’ìŒ: ê°€ë¡œë¥¼ ë ˆì´ì–´ì— ë§ì¶”ê³  ì„¸ë¡œëŠ” ë„˜ì¹¨
        let scaledWidth = layerBounds.width
        let scaledHeight = scaledWidth / videoAspectRatio
        let offsetY = (layerBounds.height - scaledHeight) / 2
        videoRect = CGRect(x: 0, y: offsetY, width: scaledWidth, height: scaledHeight)
      }
      
    case .resizeAspect:
      // Aspect Fit: ë¹„ë””ì˜¤ ë¹„ìœ¨ ìœ ì§€í•˜ë©´ì„œ ë ˆì´ì–´ ë‚´ë¶€ì— ë§ì¶¤ (ê²€ì€ ì—¬ë°± ê°€ëŠ¥)
      videoRect = AVMakeRect(aspectRatio: actualVideoSize, insideRect: layerBounds)
      
    case .resize:
      // ë¹„ìœ¨ ë¬´ì‹œí•˜ê³  ë ˆì´ì–´ ì „ì²´ë¥¼ ì±„ì›€
      videoRect = layerBounds
      
    default:
      videoRect = layerBounds
    }
    
    logDebug("ê³„ì‚°ëœ ì‹¤ì œ ë¹„ë””ì˜¤ ì˜ì—­: \(videoRect)", category: .camera)
    return videoRect
  }
  
  /// ì†¡ì¶œìš© ê³ í•´ìƒë„ UIë§Œ ë Œë”ë§ (ì¹´ë©”ë¼ í”„ë ˆì„ ì—†ì„ ë•Œ)
  /// 
  /// **1:1 â†’ 16:9 ë¹„ìœ¨ ê°•ì œ ë³€í™˜ ì ìš©**
  /// - Parameter streamingSize: ì†¡ì¶œ ëª©í‘œ í•´ìƒë„ (1920x1080)
  /// - Returns: ê³ í•´ìƒë„ UI ì´ë¯¸ì§€ ë˜ëŠ” nil
  private func renderUIOnlyForStreaming(streamingSize: CGSize) -> UIImage? {
    let currentSize = bounds.size
    guard currentSize.width > 0 && currentSize.height > 0 else { 
      logError("ìœ íš¨í•˜ì§€ ì•Šì€ ë·° í¬ê¸°: \(currentSize)", category: .performance)
      return nil 
    }
    
    // ì›ë³¸ UI ë¹„ìœ¨ ê³„ì‚°
    let originalAspectRatio = currentSize.width / currentSize.height
    let targetAspectRatio = streamingSize.width / streamingSize.height
    
    logDebug("ë¹„ìœ¨ ë¶„ì„:", category: .performance)
    logDebug("  â€¢ ì›ë³¸ UI: \(currentSize) (ë¹„ìœ¨: \(String(format: "%.2f", originalAspectRatio)))", category: .performance)
    logDebug("  â€¢ ëª©í‘œ ì†¡ì¶œ: \(streamingSize) (ë¹„ìœ¨: \(String(format: "%.2f", targetAspectRatio)))", category: .performance)
    
    // **Aspect Fill ë°©ì‹**: í™”ë©´ì„ ê½‰ ì±„ìš°ê¸° ìœ„í•´ max ì‚¬ìš© (1:1 ë¬¸ì œ í•´ê²°)
    let scaleX = streamingSize.width / currentSize.width
    let scaleY = streamingSize.height / currentSize.height
    let scale = max(scaleX, scaleY) // Aspect Fill - í™”ë©´ ê½‰ ì±„ìš°ê¸°
    
    logDebug("  â€¢ ìŠ¤ì¼€ì¼ë§: scaleX=\(String(format: "%.2f", scaleX)), scaleY=\(String(format: "%.2f", scaleY))", category: .performance)
    logDebug("  â€¢ Aspect Fill ìµœì¢… ìŠ¤ì¼€ì¼: \(String(format: "%.2f", scale))x", category: .performance)
    
    // 1:1 ë¹„ìœ¨ ë¬¸ì œ ê°ì§€ ê²½ê³  (ê°œì„ ëœ ê°ì§€)
    if abs(originalAspectRatio - 1.0) < 0.2 { // 0.8~1.2 ì‚¬ì´ëŠ” ì •ì‚¬ê°í˜•ìœ¼ë¡œ ê°„ì£¼
      logWarning("1:1 ë¬¸ì œ ê°ì§€ - ì›ë³¸ UIê°€ ì •ì‚¬ê°í˜•ì— ê°€ê¹Œì›€ (ë¹„ìœ¨: \(String(format: "%.2f", originalAspectRatio))) â†’ Aspect Fillë¡œ 16:9 ë³€í™˜", category: .performance)
    }
    
    let renderer = UIGraphicsImageRenderer(size: streamingSize)
    return renderer.image { context in
      // ë°°ê²½ì„ ê²€ì€ìƒ‰ìœ¼ë¡œ ì±„ìš°ê¸° (ì¹´ë©”ë¼ í”„ë ˆì„ì´ ì—†ì„ ë•Œ)
      context.cgContext.setFillColor(UIColor.black.cgColor)
      context.cgContext.fill(CGRect(origin: .zero, size: streamingSize))
      
      // Aspect Fill ìŠ¤ì¼€ì¼ë§ìœ¼ë¡œ UI ë Œë”ë§ (í™”ë©´ ê½‰ ì±„ìš°ê¸°)
      context.cgContext.scaleBy(x: scale, y: scale)
      
      // UIê°€ ì˜ë¦´ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì¤‘ì•™ ì •ë ¬
      let scaledSize = CGSize(width: currentSize.width * scale, height: currentSize.height * scale)
      let offsetX = (streamingSize.width - scaledSize.width) / 2.0
      let offsetY = (streamingSize.height - scaledSize.height) / 2.0
      context.cgContext.translateBy(x: offsetX / scale, y: offsetY / scale)
      
      layer.render(in: context.cgContext)
      
      logDebug("Aspect Fill ë Œë”ë§ ì™„ë£Œ: \(originalAspectRatio) â†’ \(targetAspectRatio)", category: .performance)
    }
  }

  /// ë‹¨ë§ í‘œì‹œìš© ì¹´ë©”ë¼ í”„ë ˆì„ê³¼ UI í•©ì„± (ê¸°ì¡´ ë°©ì‹ ìœ ì§€)
  /// 
  /// ì´ ë©”ì„œë“œëŠ” ë‹¤ìŒ 3ë‹¨ê³„ë¡œ ì´ë¯¸ì§€ë¥¼ í•©ì„±í•©ë‹ˆë‹¤:
  /// 1. CVPixelBuffer(ì¹´ë©”ë¼ í”„ë ˆì„)ë¥¼ UIImageë¡œ ë³€í™˜
  /// 2. UI ì„œë¸Œë·°ë“¤ì„ ë³„ë„ ì´ë¯¸ì§€ë¡œ ë Œë”ë§ (ì˜¤ë²„ë ˆì´)
  /// 3. ì¹´ë©”ë¼ ì´ë¯¸ì§€ ìœ„ì— UI ì˜¤ë²„ë ˆì´ë¥¼ í•©ì„±
  ///
  /// **í•©ì„± ë°©ì‹:**
  /// - ì¹´ë©”ë¼ ì´ë¯¸ì§€: aspect fillë¡œ ë°°ì¹˜ (ë¹„ìœ¨ ìœ ì§€í•˜ë©´ì„œ í™”ë©´ ì „ì²´ ì±„ì›€)
  /// - UI ì˜¤ë²„ë ˆì´: ì „ì²´ í™”ë©´ì— normal ë¸”ë Œë“œ ëª¨ë“œë¡œ í•©ì„±
  ///
  /// - Parameter cameraFrame: ì‹¤ì‹œê°„ ì¹´ë©”ë¼ í”„ë ˆì„ (CVPixelBuffer)
  /// - Parameter viewSize: ìµœì¢… ì¶œë ¥ ì´ë¯¸ì§€ í¬ê¸° (ë‹¨ë§ í™”ë©´ í¬ê¸°)
  /// - Returns: í•©ì„±ëœ ìµœì¢… ì´ë¯¸ì§€ ë˜ëŠ” nil
  private func renderCameraFrameWithUI(cameraFrame: CVPixelBuffer, viewSize: CGSize) -> UIImage? {
    
    // Step 1: ì¹´ë©”ë¼ í”„ë ˆì„ì„ UIImageë¡œ ë³€í™˜
    guard let cameraImage = cameraFrame.toUIImage() else {
      logError("ì¹´ë©”ë¼ í”„ë ˆì„ â†’ UIImage ë³€í™˜ ì‹¤íŒ¨", category: .performance)
      return nil
    }
    logDebug("ì¹´ë©”ë¼ ì´ë¯¸ì§€ ë³€í™˜ ì„±ê³µ: \(cameraImage.size)", category: .performance)
    
    // Step 2: UI ì˜¤ë²„ë ˆì´ ìƒì„± (ì¹´ë©”ë¼ í”„ë¦¬ë·° ë ˆì´ì–´ ì œì™¸)
    // ëª¨ë“  ì„œë¸Œë·°(ë²„íŠ¼, ë¼ë²¨, ì›Œí„°ë§ˆí¬ ë“±)ë¥¼ ë³„ë„ ì´ë¯¸ì§€ë¡œ ë Œë”ë§
    let uiRenderer = UIGraphicsImageRenderer(size: viewSize)
    let uiOverlay = uiRenderer.image { context in
      // í”„ë¦¬ë·° ë ˆì´ì–´ë¥¼ ì œì™¸í•œ ëª¨ë“  ì„œë¸Œë·° ë Œë”ë§
      // (ì¹´ë©”ë¼ í”„ë¦¬ë·°ëŠ” ì´ë¯¸ cameraImageì— í¬í•¨ë˜ì–´ ìˆìŒ)
      for subview in subviews {
        subview.layer.render(in: context.cgContext)
      }
    }
    logDebug("UI ì˜¤ë²„ë ˆì´ ìƒì„± ì™„ë£Œ", category: .performance)
    
    // Step 3: ì¹´ë©”ë¼ ì´ë¯¸ì§€ì™€ UI ì˜¤ë²„ë ˆì´ í•©ì„±
    let finalRenderer = UIGraphicsImageRenderer(size: viewSize)
    let compositeImage = finalRenderer.image { context in
      let rect = CGRect(origin: .zero, size: viewSize)
      
      // 3-1: ì¹´ë©”ë¼ ì´ë¯¸ì§€ë¥¼ ë·° í¬ê¸°ì— ë§ê²Œ ê·¸ë¦¬ê¸° (aspect fill ì ìš©)
      // Aspect Fill: ì›ë³¸ ë¹„ìœ¨ì„ ìœ ì§€í•˜ë©´ì„œ ì „ì²´ ì˜ì—­ì„ ì±„ì›€ (ì¼ë¶€ ì˜ë¦¼ ê°€ëŠ¥í•˜ì§€ë§Œ í™”ë©´ ê½‰ ì±„ì›€)
      let cameraAspectRatio = cameraImage.size.width / cameraImage.size.height
      let rectAspectRatio = rect.width / rect.height
      
      let drawRect: CGRect
      if cameraAspectRatio > rectAspectRatio {
        // ì¹´ë©”ë¼ê°€ ë” ë„“ìŒ: ë†’ì´ë¥¼ ë§ì¶”ê³  ê°€ë¡œëŠ” ë„˜ì¹¨
        let drawHeight = rect.height
        let drawWidth = drawHeight * cameraAspectRatio
        let offsetX = (rect.width - drawWidth) / 2
        drawRect = CGRect(x: offsetX, y: 0, width: drawWidth, height: drawHeight)
      } else {
        // ì¹´ë©”ë¼ê°€ ë” ë†’ìŒ: ë„ˆë¹„ë¥¼ ë§ì¶”ê³  ì„¸ë¡œëŠ” ë„˜ì¹¨
        let drawWidth = rect.width
        let drawHeight = drawWidth / cameraAspectRatio
        let offsetY = (rect.height - drawHeight) / 2
        drawRect = CGRect(x: 0, y: offsetY, width: drawWidth, height: drawHeight)
      }
      
      cameraImage.draw(in: drawRect)
      
      // 3-2: UI ì˜¤ë²„ë ˆì´ë¥¼ ì „ì²´ í™”ë©´ì— í•©ì„±
      // normal ë¸”ë Œë“œ ëª¨ë“œ: íˆ¬ëª… ì˜ì—­ì€ ê·¸ëŒ€ë¡œ ë‘ê³  ë¶ˆíˆ¬ëª… ì˜ì—­ë§Œ ë®ì–´ì”€
      uiOverlay.draw(in: rect, blendMode: .normal, alpha: 1.0)
    }
    
    logDebug("ìµœì¢… ì´ë¯¸ì§€ í•©ì„± ì™„ë£Œ: \(viewSize)", category: .performance)
    return compositeImage
  }

  /// CVPixelBufferë¥¼ HaishinKitì— ì „ë‹¬í•˜ì—¬ ìŠ¤íŠ¸ë¦¬ë°
  /// 
  /// ìº¡ì²˜ëœ í”„ë ˆì„ì„ HaishinKitì˜ ìˆ˜ë™ í”„ë ˆì„ ì „ì†¡ ê¸°ëŠ¥ì„ í†µí•´
  /// ìŠ¤íŠ¸ë¦¬ë° ì„œë²„ë¡œ ì „ì†¡í•©ë‹ˆë‹¤.
  ///
  /// **ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§:**
  /// - 5ì´ˆë§ˆë‹¤ ì „ì†¡ í†µê³„ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤
  /// - ì„±ê³µ/ì‹¤íŒ¨ ì¹´ìš´íŠ¸ì™€ í˜„ì¬ FPSë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
  ///
  /// - Parameter pixelBuffer: ì „ì†¡í•  í”„ë ˆì„ ë°ì´í„°
  private func sendFrameToHaishinKit(_ pixelBuffer: CVPixelBuffer) {
    // ì„±ëŠ¥ ìµœì í™”: í”„ë ˆì„ë³„ ì „ì†¡ ë¡œê·¸ ì œê±° (CPU ë¶€í•˜ ê°ì†Œ)
    // let width = CVPixelBufferGetWidth(pixelBuffer)
    // let height = CVPixelBufferGetHeight(pixelBuffer)
            // print("ğŸ“¡ [ì „ì†¡] HaishinKit í”„ë ˆì„ ì „ë‹¬: \(width)x\(height)") // ì´ë¯¸ ë¹„í™œì„±í™”ë¨

    // HaishinKitManagerë¥¼ í†µí•œ ì‹¤ì œ í”„ë ˆì„ ì „ì†¡
    if let manager = haishinKitManager {
      Task {
        await manager.sendManualFrame(pixelBuffer)
      }

      // ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§: 5ì´ˆë§ˆë‹¤ ì „ì†¡ í†µê³„ ì¶œë ¥ (25fps ê¸°ì¤€)
      if frameCounter % 125 == 0 { // 25fps ê¸°ì¤€ 5ì´ˆë§ˆë‹¤ = 125í”„ë ˆì„ë§ˆë‹¤
        let stats = manager.getScreenCaptureStats()
        let successRate = stats.frameCount > 0 ? (Double(stats.successCount) / Double(stats.frameCount)) * 100 : 0
        logInfo("""
        í™”ë©´ìº¡ì²˜ í†µê³„ 
        - í˜„ì¬ FPS: \(String(format: "%.1f", stats.currentFPS))
        - ì„±ê³µ ì „ì†¡: \(stats.successCount)í”„ë ˆì„
        - ì‹¤íŒ¨ ì „ì†¡: \(stats.failureCount)í”„ë ˆì„
        - ì„±ê³µë¥ : \(String(format: "%.1f", successRate))%
        - ì´ ì²˜ë¦¬: \(stats.frameCount)í”„ë ˆì„
        """, category: .performance)
      }
      frameCounter += 1
    } else {
      logWarning("HaishinKitManager ì—†ìŒ - í”„ë ˆì„ ì „ë‹¬ ë¶ˆê°€", category: .streaming)
    }
  }

  /// ì†¡ì¶œ í•´ìƒë„ì— ë”°ë¥¸ ìµœì  ìº¡ì²˜ ì‚¬ì´ì¦ˆ ê³„ì‚° (16:9 ë¹„ìœ¨ ê³ ì •)
  /// 
  /// **16:9 ë¹„ìœ¨ ê°•ì œ ì ìš©:**
  /// - 480p(854x480) â†’ 16:9 ë¹„ìœ¨ë¡œ ìˆ˜ì • í›„ 2ë°° ì—…ìŠ¤ì¼€ì¼
  /// - 720p(1280x720) â†’ 2ë°° ì—…ìŠ¤ì¼€ì¼  
  /// - 1080p(1920x1080) â†’ ë™ì¼ í•´ìƒë„ ìº¡ì²˜
  /// - ëª¨ë“  í•´ìƒë„ë¥¼ 16:9 ë¹„ìœ¨ë¡œ ê°•ì œ ë³€í™˜
  /// 
  /// - Returns: 16:9 ë¹„ìœ¨ì´ ë³´ì¥ëœ ìµœì  ìº¡ì²˜ í•´ìƒë„
  private func getOptimalCaptureSize() -> CGSize {
    // HaishinKitManagerì—ì„œ í˜„ì¬ ìŠ¤íŠ¸ë¦¬ë° ì„¤ì • ê°€ì ¸ì˜¤ê¸°
    guard let manager = haishinKitManager,
          let settings = manager.getCurrentSettings() else {
      // ê¸°ë³¸ê°’: 720p (16:9 ë¹„ìœ¨)
      return CGSize(width: 1280, height: 720)
    }
    
    let streamWidth = settings.videoWidth
    let streamHeight = settings.videoHeight
    
    // 16:9 ë¹„ìœ¨ ê°•ì œ ì ìš© (ìœ íŠœë¸Œ ë¼ì´ë¸Œ í‘œì¤€)
    let aspectRatio: CGFloat = 16.0 / 9.0
    
    // ì†¡ì¶œ í•´ìƒë„ë¥¼ 16:9 ë¹„ìœ¨ë¡œ ìˆ˜ì •
    let correctedStreamSize: CGSize
    let currentAspectRatio = CGFloat(streamWidth) / CGFloat(streamHeight)
    
    if abs(currentAspectRatio - aspectRatio) > 0.1 {
      // ë¹„ìœ¨ì´ 16:9ê°€ ì•„ë‹ˆë©´ ê°•ì œë¡œ ìˆ˜ì •
      let correctedHeight = CGFloat(streamWidth) / aspectRatio
      correctedStreamSize = CGSize(width: streamWidth, height: Int(correctedHeight))
      logInfo("ë¹„ìœ¨ìˆ˜ì •: \(streamWidth)x\(streamHeight) (ë¹„ìœ¨: \(String(format: "%.2f", currentAspectRatio))) â†’ \(correctedStreamSize) (16:9)", category: .streaming)
    } else {
      correctedStreamSize = CGSize(width: streamWidth, height: streamHeight)
      logDebug("ì´ë¯¸ 16:9 ë¹„ìœ¨: \(correctedStreamSize)", category: .streaming)
    }
    
    // 16:9 ë¹„ìœ¨ ê¸°ë°˜ ìµœì  ìº¡ì²˜ í•´ìƒë„ ê³„ì‚°
    let captureSize: CGSize
    let width = Int(correctedStreamSize.width)
    let height = Int(correctedStreamSize.height)
    
    switch (width, height) {
    case (640...854, 360...480):
      // 480p ê³„ì—´ â†’ 2ë°° ì—…ìŠ¤ì¼€ì¼
      captureSize = CGSize(width: 1280, height: 720) // 720pë¡œ ìº¡ì²˜
      logDebug("16:9 ìº¡ì²˜ - 480pê³„ì—´ ì†¡ì¶œ â†’ 720p ìº¡ì²˜: \(captureSize)", category: .streaming)
      
    case (1280, 720):
      // 720p â†’ 2ë°° ì—…ìŠ¤ì¼€ì¼
      captureSize = CGSize(width: 2560, height: 1440)
      logDebug("16:9 ìº¡ì²˜ - 720p ì†¡ì¶œ â†’ 1440p ìº¡ì²˜: \(captureSize)", category: .streaming)
      
    case (1920, 1080):
      // 1080p â†’ ë™ì¼ í•´ìƒë„ (ì•ˆì •ì„± ìš°ì„ )
      captureSize = CGSize(width: 1920, height: 1080)
      logDebug("16:9 ìº¡ì²˜ - 1080p ì†¡ì¶œ â†’ 1080p ìº¡ì²˜: \(captureSize)", category: .streaming)
      
    default:
      // ì‚¬ìš©ì ì •ì˜ â†’ 16:9 ë¹„ìœ¨ë¡œ ê°•ì œ ë³€í™˜ í›„ ìº¡ì²˜
      let targetWidth = max(width, 1280) // ìµœì†Œ 720p ë„ˆë¹„
      let targetHeight = Int(CGFloat(targetWidth) / aspectRatio)
      captureSize = CGSize(width: targetWidth, height: targetHeight)
      logDebug("16:9 ìº¡ì²˜ - ì‚¬ìš©ìì •ì˜ â†’ 16:9 ê°•ì œë³€í™˜ ìº¡ì²˜: \(captureSize)", category: .streaming)
    }
    
    // 16ì˜ ë°°ìˆ˜ë¡œ ì •ë ¬ (VideoCodec í˜¸í™˜ì„±)
    let alignedWidth = ((Int(captureSize.width) + 15) / 16) * 16
    let alignedHeight = ((Int(captureSize.height) + 15) / 16) * 16
    let finalSize = CGSize(width: alignedWidth, height: alignedHeight)
    
    // ìµœì¢… 16:9 ë¹„ìœ¨ ê²€ì¦
    let finalAspectRatio = CGFloat(alignedWidth) / CGFloat(alignedHeight)
    logDebug("ìµœì¢…ê²€ì¦ - 16ë°°ìˆ˜ ì •ë ¬: \(captureSize) â†’ \(finalSize)", category: .streaming)
    logDebug("ìµœì¢…ê²€ì¦ - ë¹„ìœ¨ í™•ì¸: \(String(format: "%.2f", finalAspectRatio)) (16:9 â‰ˆ 1.78)", category: .streaming)
    
    return finalSize
  }

  /// í™”ë©´ ìº¡ì²˜ ìƒíƒœì™€ í†µê³„ í™•ì¸ (ê³µê°œ ë©”ì„œë“œ)
  public func getScreenCaptureStatus() -> (isCapturing: Bool, stats: String?) {
    let stats = haishinKitManager?.getScreenCaptureStats()
    return (isScreenCapturing, stats?.summary)
  }

  /// í™”ë©´ ìº¡ì²˜ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
  public func testScreenCapturePerformance() {
    guard let manager = haishinKitManager else {
      logError("HaishinKitManagerê°€ ì—†ìŒ", category: .streaming)
      return
    }

    logInfo("í™”ë©´ ìº¡ì²˜ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘...", category: .performance)
    manager.resetScreenCaptureStats()

    // 10í”„ë ˆì„ ì—°ì† ì „ì†¡ í…ŒìŠ¤íŠ¸
    for i in 1...10 {
      DispatchQueue.main.asyncAfter(deadline: .now() + Double(i) * 0.1) { [weak self] in
        if let image = self?.renderToImage(),
          let pixelBuffer = image.toCVPixelBuffer()
        {
          Task {
            await manager.sendManualFrame(pixelBuffer)
          }

          if i == 10 {
            DispatchQueue.main.asyncAfter(deadline: .now() + 0.5) {
              let stats = manager.getScreenCaptureStats()
              logInfo("í…ŒìŠ¤íŠ¸ ì™„ë£Œ:", category: .performance)
              logInfo(stats.summary, category: .performance)
            }
          }
        }
      }
    }
  }
  
  /// ë‹¨ë§ í‘œì‹œìš© í™”ë©´ ìº¡ì²˜ (ì‚¬ìš©ì í™”ë©´ì— í‘œì‹œìš©)
  /// 
  /// ì†¡ì¶œê³¼ ë³„ë„ë¡œ ì‚¬ìš©ìê°€ iPadì—ì„œ ë³¼ ìˆ˜ ìˆëŠ” í™”ë©´ ìº¡ì²˜ ê¸°ëŠ¥
  /// - Returns: ë‹¨ë§ í™”ë©´ í¬ê¸°ì˜ ì´ë¯¸ì§€
  public func captureForDisplay() -> UIImage? {
    return renderToImageForDisplay()
  }
  
  /// ì†¡ì¶œìš©ê³¼ ë‹¨ë§ìš© ì´ë¯¸ì§€ ë™ì‹œ ìƒì„±
  /// 
  /// - Returns: (ì†¡ì¶œìš©: 1920x1080, ë‹¨ë§ìš©: 986x865) íŠœí”Œ
  public func captureForBothPurposes() -> (streaming: UIImage?, display: UIImage?) {
    let streamingImage = renderToImageForStreaming()
    let displayImage = renderToImageForDisplay()
    
    logDebug("ì´ì¤‘ìº¡ì²˜ - ì†¡ì¶œìš©: \(streamingImage?.size ?? CGSize.zero), ë‹¨ë§ìš©: \(displayImage?.size ?? CGSize.zero)", category: .performance)
    
    return (streamingImage, displayImage)
  }
  
  /// ë‹¨ë§ í™”ë©´ ìº¡ì²˜ ì €ì¥ (ì‚¬ì§„ ì•±ì— ì €ì¥)
  /// 
  /// ì‚¬ìš©ìê°€ í˜„ì¬ í™”ë©´ì„ ì‚¬ì§„ìœ¼ë¡œ ì €ì¥í•  ë•Œ ì‚¬ìš©
  public func saveDisplayCapture(completion: @escaping (Bool, Error?) -> Void) {
    guard let displayImage = renderToImageForDisplay() else {
      completion(false, NSError(domain: "CameraPreview", code: 1, userInfo: [NSLocalizedDescriptionKey: "í™”ë©´ ìº¡ì²˜ ì‹¤íŒ¨"]))
      return
    }
    
    UIImageWriteToSavedPhotosAlbum(displayImage, nil, nil, nil)
    logInfo("í™”ë©´ ìº¡ì²˜ ì‚¬ì§„ ì•±ì— ì €ì¥ ì™„ë£Œ: \(displayImage.size)", category: .general)
    completion(true, nil)
  }

  /// í™”ë©´ ìº¡ì²˜ ìƒíƒœ í™•ì¸
  var isCapturingScreen: Bool {
    return isScreenCapturing
  }

  // MARK: - Usage Example & Notes

  /*
   ì‚¬ìš© ì˜ˆì‹œ:
  
   1. ì¼ë°˜ ì¹´ë©”ë¼ ìŠ¤íŠ¸ë¦¬ë°: (ì œê±°ë¨ - í™”ë©´ ìº¡ì²˜ ìŠ¤íŠ¸ë¦¬ë°ë§Œ ì‚¬ìš©)
      // try await haishinKitManager.startStreaming(with: settings, captureSession: captureSession)
  
   2. í™”ë©´ ìº¡ì²˜ ìŠ¤íŠ¸ë¦¬ë°:
      // Step 1: í™”ë©´ ìº¡ì²˜ ëª¨ë“œë¡œ ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘
      try await haishinKitManager.startScreenCaptureStreaming(with: settings)
  
      // Step 2: CameraPreviewUIViewì—ì„œ í™”ë©´ ìº¡ì²˜ ì‹œì‘
      cameraPreviewUIView.startScreenCapture()
  
      // Step 3: ì¤‘ì§€í•  ë•Œ
      cameraPreviewUIView.stopScreenCapture()
      await haishinKitManager.stopStreaming()
  
   ì£¼ì˜ì‚¬í•­:
   - í™”ë©´ ìº¡ì²˜ëŠ” 30fpsë¡œ ë™ì‘í•˜ë¯€ë¡œ ì„±ëŠ¥ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤
   - UIView ë Œë”ë§ì€ ë©”ì¸ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰ë˜ë¯€ë¡œ UI ë¸”ë¡œí‚¹ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤
   - í™”ë©´ì— ë³´ì´ëŠ” ëª¨ë“  UI ìš”ì†Œ(ë²„íŠ¼, ë¼ë²¨ ë“±)ê°€ ì†¡ì¶œì— í¬í•¨ë©ë‹ˆë‹¤
   - ì‹¤ì œ HaishinKit manual capture êµ¬í˜„ì€ ì¶”ê°€ ì‘ì—…ì´ í•„ìš”í•©ë‹ˆë‹¤
   */
}

// MARK: - CameraControlOverlayDelegate

extension CameraPreviewUIView: CameraControlOverlayDelegate {
  func didTapRecord() {
    // ë…¹í™” ê¸°ëŠ¥ì€ ì œì™¸
    logInfo("Recording functionality not implemented", category: .general)
  }
}

// MARK: - AVCaptureVideoDataOutputSampleBufferDelegate

extension CameraPreviewUIView: AVCaptureVideoDataOutputSampleBufferDelegate {
  /// AVCaptureVideoDataOutputì—ì„œ í”„ë ˆì„ì„ ë°›ëŠ” ë¸ë¦¬ê²Œì´íŠ¸ ë©”ì„œë“œ
  /// 
  /// **ì´ ë©”ì„œë“œì˜ ë‘ ê°€ì§€ ì—­í• :**
  /// 1. **í™”ë©´ ìº¡ì²˜ ëª¨ë“œ**: ì‹¤ì‹œê°„ ì¹´ë©”ë¼ í”„ë ˆì„ì„ ì €ì¥í•˜ì—¬ UIì™€ í•©ì„±
  /// 2. **ì¼ë°˜ ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ**: HaishinKitì— í”„ë ˆì„ í†µê³„ ì •ë³´ ì „ë‹¬
  ///
  /// **ì„±ëŠ¥ ìµœì í™”:**
  /// - í™”ë©´ ìº¡ì²˜ ì¤‘ì¼ ë•Œë§Œ í”„ë ˆì„ì„ ì €ì¥í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì†Œí™”
  /// - ë°±ê·¸ë¼ìš´ë“œ íì—ì„œ í”„ë ˆì„ ì €ì¥ ì‘ì—… ìˆ˜í–‰í•˜ì—¬ ë©”ì¸ ìŠ¤ë ˆë“œ ë¸”ë¡í‚¹ ë°©ì§€
  ///
  /// - Parameter output: ì¶œë ¥ ê°ì²´ (AVCaptureVideoDataOutput)
  /// - Parameter sampleBuffer: ì¹´ë©”ë¼ì—ì„œ ìº¡ì²˜ëœ í”„ë ˆì„ ë°ì´í„°
  /// - Parameter connection: ì…ë ¥ê³¼ ì¶œë ¥ ê°„ì˜ ì—°ê²° ì •ë³´
  func captureOutput(
    _ output: AVCaptureOutput, didOutput sampleBuffer: CMSampleBuffer,
    from connection: AVCaptureConnection
  ) {
    // ğŸ¬ í™”ë©´ ìº¡ì²˜ ëª¨ë“œ: ì‹¤ì‹œê°„ ì¹´ë©”ë¼ í”„ë ˆì„ ì €ì¥
    // UIì™€ í•©ì„±í•˜ê¸° ìœ„í•´ ìµœì‹  í”„ë ˆì„ì„ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì €ì¥
    if isScreenCapturing {
      guard let pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer) else { 
        logWarning("CMSampleBufferì—ì„œ pixelBuffer ì¶”ì¶œ ì‹¤íŒ¨", category: .camera)
        return 
      }
      
      // ë°±ê·¸ë¼ìš´ë“œ íì—ì„œ í”„ë ˆì„ ì €ì¥ (ë©”ì¸ ìŠ¤ë ˆë“œ ë¸”ë¡í‚¹ ë°©ì§€)
      frameProcessingQueue.async { [weak self] in
        self?.latestCameraFrame = pixelBuffer
        // print("âœ… [í”„ë ˆì„ì €ì¥] ìµœì‹  ì¹´ë©”ë¼ í”„ë ˆì„ ì—…ë°ì´íŠ¸ë¨") // ë°˜ë³µì ì¸ ë¡œê·¸ ë¹„í™œì„±í™”
      }
    }
    
    // ğŸ“¡ ì¼ë°˜ ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ: HaishinKitì— í”„ë ˆì„ í†µê³„ ì „ë‹¬
    // í™”ë©´ ìº¡ì²˜ê°€ ì•„ë‹Œ ì¼ë°˜ ì¹´ë©”ë¼ ìŠ¤íŠ¸ë¦¬ë° ì‹œ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ìš©
    guard isStreaming, let manager = haishinKitManager else { return }

    // HaishinKitì— í”„ë ˆì„ í†µê³„ ì •ë³´ ì „ë‹¬ (ë¹„ë™ê¸° ì²˜ë¦¬)
    Task {
      await manager.processVideoFrame(sampleBuffer)
    }
  }

  func captureOutput(
    _ output: AVCaptureOutput, didDrop sampleBuffer: CMSampleBuffer,
    from connection: AVCaptureConnection
  ) {
            // í”„ë ˆì„ ë“œë¡­ì€ ì •ìƒì ì¸ í˜„ìƒì´ë¯€ë¡œ ë¡œê·¸ ë¹„í™œì„±í™”
        // print("âš ï¸ [CameraPreview] ë¹„ë””ì˜¤ í”„ë ˆì„ ë“œë¡­ë¨ - ì„±ëŠ¥ ìµœì í™” í•„ìš”í•  ìˆ˜ ìˆìŒ")
  }
}

// MARK: - Supporting Views

/// ì¹´ë©”ë¼ ì»¨íŠ¸ë¡¤ ì˜¤ë²„ë ˆì´
protocol CameraControlOverlayDelegate: AnyObject {
  func didTapRecord()
}

final class CameraControlOverlay: UIView {
  weak var delegate: CameraControlOverlayDelegate?

  override init(frame: CGRect) {
    super.init(frame: frame)
    setupView()
  }

  required init?(coder: NSCoder) {
    super.init(coder: coder)
    setupView()
  }

  private func setupView() {
    // ë²„íŠ¼ë“¤ì„ ì œê±°í–ˆìœ¼ë¯€ë¡œ ë¹ˆ ë·°ë¡œ ì„¤ì •
  }
}

/// ìŠ¤íŠ¸ë¦¬ë° ìƒíƒœ í‘œì‹œ ë·°
final class StreamingStatusView: UIView {

  private lazy var containerView: UIView = {
    let view = UIView()
    view.backgroundColor = UIColor.red.withAlphaComponent(0.8)
    view.layer.cornerRadius = 8
    view.translatesAutoresizingMaskIntoConstraints = false
    return view
  }()

  private lazy var liveLabel: UILabel = {
    let label = UILabel()
    label.text = "ğŸ”´ LIVE"
    label.textColor = .white
    label.font = .boldSystemFont(ofSize: 14)
    label.translatesAutoresizingMaskIntoConstraints = false
    return label
  }()

  private lazy var statsLabel: UILabel = {
    let label = UILabel()
    label.textColor = .white
    label.font = .systemFont(ofSize: 12)
    label.numberOfLines = 2
    label.translatesAutoresizingMaskIntoConstraints = false
    return label
  }()

  override init(frame: CGRect) {
    super.init(frame: frame)
    setupView()
  }

  required init?(coder: NSCoder) {
    super.init(coder: coder)
    setupView()
  }

  private func setupView() {
    addSubview(containerView)
    containerView.addSubview(liveLabel)
    containerView.addSubview(statsLabel)

    NSLayoutConstraint.activate([
      containerView.topAnchor.constraint(equalTo: topAnchor),
      containerView.leadingAnchor.constraint(equalTo: leadingAnchor),
      containerView.trailingAnchor.constraint(equalTo: trailingAnchor),
      containerView.bottomAnchor.constraint(equalTo: bottomAnchor),

      liveLabel.topAnchor.constraint(equalTo: containerView.topAnchor, constant: 8),
      liveLabel.leadingAnchor.constraint(equalTo: containerView.leadingAnchor, constant: 8),
      liveLabel.trailingAnchor.constraint(equalTo: containerView.trailingAnchor, constant: -8),

      statsLabel.topAnchor.constraint(equalTo: liveLabel.bottomAnchor, constant: 4),
      statsLabel.leadingAnchor.constraint(equalTo: containerView.leadingAnchor, constant: 8),
      statsLabel.trailingAnchor.constraint(equalTo: containerView.trailingAnchor, constant: -8),
      statsLabel.bottomAnchor.constraint(equalTo: containerView.bottomAnchor, constant: -8),
    ])
  }

  func updateStatus(_ status: String) {
    liveLabel.text = status
  }

  /// ì¬ì—°ê²° ìƒíƒœ ì—…ë°ì´íŠ¸
  func updateReconnectingStatus(_ attempt: Int, _ maxAttempts: Int, _ delay: Int) {
    liveLabel.text = "ğŸ”„ ì¬ì—°ê²° ì¤‘"
    statsLabel.text = "ì‹œë„: \(attempt)/\(maxAttempts)\n\(delay)ì´ˆ í›„ ì¬ì‹œë„"

    // ì¬ì—°ê²° ì¤‘ì¼ ë•Œ ë°°ê²½ìƒ‰ì„ ì£¼í™©ìƒ‰ìœ¼ë¡œ ë³€ê²½
    containerView.backgroundColor = UIColor.orange.withAlphaComponent(0.8)
  }

  /// ì—°ê²° ì‹¤íŒ¨ ìƒíƒœ ì—…ë°ì´íŠ¸
  func updateFailedStatus(_ message: String) {
    liveLabel.text = "âŒ ì—°ê²° ì‹¤íŒ¨"
    statsLabel.text = message

    // ì‹¤íŒ¨ ì‹œ ë°°ê²½ìƒ‰ì„ ë¹¨ê°„ìƒ‰ìœ¼ë¡œ ë³€ê²½
    containerView.backgroundColor = UIColor.red.withAlphaComponent(0.8)
  }

  /// ì •ìƒ ìŠ¤íŠ¸ë¦¬ë° ìƒíƒœë¡œ ë³µì›
  func updateStreamingStatus() {
    liveLabel.text = "ğŸ”´ LIVE"

    // ì •ìƒ ìƒíƒœë¡œ ë°°ê²½ìƒ‰ ë³µì›
    containerView.backgroundColor = UIColor.red.withAlphaComponent(0.8)
  }

  func updateStats(_ stats: StreamStats) {
    let duration = formatDuration(Int(stats.duration))
    statsLabel.text = "\(duration)\n\(Int(stats.videoBitrate))kbps"
  }

  private func formatDuration(_ seconds: Int) -> String {
    let hours = seconds / 3600
    let minutes = (seconds % 3600) / 60
    let secs = seconds % 60

    if hours > 0 {
      return String(format: "%02d:%02d:%02d", hours, minutes, secs)
    } else {
      return String(format: "%02d:%02d", minutes, secs)
    }
  }
}

/// í¬ì»¤ìŠ¤ ì¸ë””ì¼€ì´í„° ë·°
final class FocusIndicatorView: UIView {

  override init(frame: CGRect) {
    super.init(frame: frame)
    setupView()
  }

  required init?(coder: NSCoder) {
    super.init(coder: coder)
    setupView()
  }

  private func setupView() {
    backgroundColor = .clear
    layer.borderColor = UIColor.yellow.cgColor
    layer.borderWidth = 2
    alpha = 0
  }

  func animate(completion: @escaping () -> Void) {
    UIView.animate(
      withDuration: 0.2,
      animations: {
        self.alpha = 1.0
        self.transform = CGAffineTransform(scaleX: 1.2, y: 1.2)
      }
    ) { _ in
      UIView.animate(
        withDuration: 0.3,
        animations: {
          self.alpha = 0.8
          self.transform = CGAffineTransform.identity
        }
      ) { _ in
        UIView.animate(
          withDuration: 1.0,
          animations: {
            self.alpha = 0
          },
          completion: { _ in
            completion()
          })
      }
    }
  }
}

/// ë…¸ì¶œ ì¸ë””ì¼€ì´í„° ë·°
final class ExposureIndicatorView: UIView {

  override init(frame: CGRect) {
    super.init(frame: frame)
    setupView()
  }

  required init?(coder: NSCoder) {
    super.init(coder: coder)
    setupView()
  }

  private func setupView() {
    backgroundColor = .clear
    layer.borderColor = UIColor.orange.cgColor
    layer.borderWidth = 2
    layer.cornerRadius = 30
    alpha = 0

    let sunIcon = UILabel()
    sunIcon.text = "â˜€ï¸"
    sunIcon.font = .systemFont(ofSize: 24)
    sunIcon.textAlignment = .center
    sunIcon.translatesAutoresizingMaskIntoConstraints = false
    addSubview(sunIcon)

    NSLayoutConstraint.activate([
      sunIcon.centerXAnchor.constraint(equalTo: centerXAnchor),
      sunIcon.centerYAnchor.constraint(equalTo: centerYAnchor),
    ])
  }

  func animate(completion: @escaping () -> Void) {
    UIView.animate(
      withDuration: 0.2,
      animations: {
        self.alpha = 1.0
        self.transform = CGAffineTransform(scaleX: 1.1, y: 1.1)
      }
    ) { _ in
      UIView.animate(
        withDuration: 0.3,
        animations: {
          self.alpha = 0.8
          self.transform = CGAffineTransform.identity
        }
      ) { _ in
        UIView.animate(
          withDuration: 1.0,
          animations: {
            self.alpha = 0
          },
          completion: { _ in
            completion()
          })
      }
    }
  }
}

// MARK: - Extensions

/// CVPixelBufferë¥¼ UIImageë¡œ ë³€í™˜í•˜ëŠ” í™•ì¥
/// 
/// **ìš©ë„:**
/// - ì‹¤ì‹œê°„ ì¹´ë©”ë¼ í”„ë ˆì„(CVPixelBuffer)ì„ UI í•©ì„±ì´ ê°€ëŠ¥í•œ UIImageë¡œ ë³€í™˜
/// - AVCaptureVideoDataOutputì—ì„œ ë°›ì€ í”„ë ˆì„ì„ í™”ë©´ ìº¡ì²˜ ì‹œ ì‚¬ìš©
///
/// **ë³€í™˜ ê³¼ì •:**
/// 1. CVPixelBuffer â†’ CIImage ë³€í™˜
/// 2. CIImage â†’ CGImage ë³€í™˜ (Core Graphics í˜¸í™˜)
/// 3. CGImage â†’ UIImage ë³€í™˜ (UIKit í˜¸í™˜)
extension CVPixelBuffer {
  
  /// CVPixelBufferë¥¼ UIImageë¡œ ë³€í™˜
  /// 
  /// Core Image í”„ë ˆì„ì›Œí¬ë¥¼ ì‚¬ìš©í•˜ì—¬ í”½ì…€ ë²„í¼ë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
  /// ì´ ê³¼ì •ì€ GPU ê°€ì†ì„ í™œìš©í•˜ì—¬ íš¨ìœ¨ì ìœ¼ë¡œ ìˆ˜í–‰ë©ë‹ˆë‹¤.
  ///
  /// **ì„±ëŠ¥ ê³ ë ¤ì‚¬í•­:**
  /// - CIContextëŠ” GPU ë¦¬ì†ŒìŠ¤ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ì¬ì‚¬ìš© ê¶Œì¥
  /// - í˜„ì¬ëŠ” ë§¤ë²ˆ ìƒˆë¡œ ìƒì„±í•˜ì§€ë§Œ, í–¥í›„ ìºì‹± ìµœì í™” ê°€ëŠ¥
  ///
  /// - Returns: ë³€í™˜ëœ UIImage ë˜ëŠ” ë³€í™˜ ì‹¤íŒ¨ ì‹œ nil
  func toUIImage() -> UIImage? {
    // Step 1: CVPixelBufferë¥¼ CIImageë¡œ ë³€í™˜
    // Core Imageê°€ í”½ì…€ ë²„í¼ë¥¼ ì§ì ‘ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë³€í™˜
    let ciImage = CIImage(cvPixelBuffer: self)
    
    // Step 2: CIContext ìƒì„± (GPU ê°€ì† í™œìš©)
    // TODO: ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•´ ì „ì—­ CIContext ìºì‹± ê³ ë ¤
    let context = CIContext()
    
    // Step 3: CIImageë¥¼ CGImageë¡œ ë³€í™˜
    // extent: ì´ë¯¸ì§€ì˜ ì „ì²´ ì˜ì—­ì„ ì˜ë¯¸ (ì›ë³¸ í¬ê¸° ìœ ì§€)
    guard let cgImage = context.createCGImage(ciImage, from: ciImage.extent) else {
      print("âŒ [CVPixelBuffer] CIImage â†’ CGImage ë³€í™˜ ì‹¤íŒ¨")
      return nil
    }
    
    // Step 4: CGImageë¥¼ UIImageë¡œ ë³€í™˜ (UIKit í˜¸í™˜)
    // ìµœì¢…ì ìœ¼ë¡œ UIKitì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜ ì™„ë£Œ
    return UIImage(cgImage: cgImage)
  }
}

/// UIImageë¥¼ CVPixelBufferë¡œ ë³€í™˜í•˜ëŠ” í™•ì¥
extension UIImage {
  func toCVPixelBuffer() -> CVPixelBuffer? {
    let attrs =
      [
        kCVPixelBufferCGImageCompatibilityKey: kCFBooleanTrue as Any,
        kCVPixelBufferCGBitmapContextCompatibilityKey: kCFBooleanTrue as Any,
        kCVPixelBufferIOSurfacePropertiesKey: [:],
      ] as CFDictionary

    var pixelBuffer: CVPixelBuffer?

    // BGRA í¬ë§· ì‚¬ìš© (HaishinKitê³¼ í˜¸í™˜ì„± í–¥ìƒ)
    let status = CVPixelBufferCreate(
      kCFAllocatorDefault,
      Int(size.width),
      Int(size.height),
      kCVPixelFormatType_32BGRA,
      attrs,
      &pixelBuffer
    )

    guard status == kCVReturnSuccess, let buffer = pixelBuffer else {
      print("âŒ [CVPixelBuffer] ìƒì„± ì‹¤íŒ¨: \(status)")
      return nil
    }

    CVPixelBufferLockBaseAddress(buffer, CVPixelBufferLockFlags(rawValue: 0))
    defer {
      CVPixelBufferUnlockBaseAddress(buffer, CVPixelBufferLockFlags(rawValue: 0))
    }

    let pixelData = CVPixelBufferGetBaseAddress(buffer)
    let rgbColorSpace = CGColorSpaceCreateDeviceRGB()
    let bytesPerRow = CVPixelBufferGetBytesPerRow(buffer)

    // BGRA í¬ë§·ì— ë§ëŠ” ì»¨í…ìŠ¤íŠ¸ ìƒì„±
    guard
      let context = CGContext(
        data: pixelData,
        width: Int(size.width),
        height: Int(size.height),
        bitsPerComponent: 8,
        bytesPerRow: bytesPerRow,
        space: rgbColorSpace,
        bitmapInfo: CGImageAlphaInfo.noneSkipFirst.rawValue
          | CGBitmapInfo.byteOrder32Little.rawValue
      )
    else {
      print("âŒ [CVPixelBuffer] CGContext ìƒì„± ì‹¤íŒ¨")
      return nil
    }

    // ì´ë¯¸ì§€ ê·¸ë¦¬ê¸° (Yì¶• ë’¤ì§‘ê¸°)
    context.translateBy(x: 0, y: size.height)
    context.scaleBy(x: 1.0, y: -1.0)

    UIGraphicsPushContext(context)
    draw(in: CGRect(x: 0, y: 0, width: size.width, height: size.height))
    UIGraphicsPopContext()

    print("âœ… [CVPixelBuffer] ìƒì„± ì„±ê³µ: \(Int(size.width))x\(Int(size.height)) BGRA")
    return buffer
  }
}

